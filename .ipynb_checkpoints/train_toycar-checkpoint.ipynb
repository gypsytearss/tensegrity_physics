{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "\n",
      "Requirements:\n",
      " - Caffe (script to install Caffe and pycaffe on a new Ubuntu 14.04 LTS x64 or Ubuntu 14.10 x64. \n",
      "   CPU only, multi-threaded Caffe. http://stackoverflow.com/a/31396229/395857)\n",
      " - sudo pip install pydot\n",
      " - sudo apt-get install -y graphviz\n",
      "\n",
      "Interesting resources on Caffe:\n",
      " - https://github.com/BVLC/caffe/tree/master/examples\n",
      " - http://nbviewer.ipython.org/github/joyofdata/joyofdata-articles/blob/master/deeplearning-with-caffe/Neural-Networks-with-Caffe-on-the-GPU.ipynb\n",
      "'''\n",
      "\n",
      "import subprocess\n",
      "import platform\n",
      "import copy\n",
      "\n",
      "# from sklearn.datasets import load_iris\n",
      "import sklearn.metrics \n",
      "import numpy as np\n",
      "from sklearn.cross_validation import StratifiedShuffleSplit\n",
      "import matplotlib.pyplot as plt\n",
      "import h5py\n",
      "import caffe\n",
      "import caffe.draw\n",
      "\n",
      "\n",
      "def load_data():\n",
      "    '''\n",
      "    Load Sample for Forward Pass from Toy Car Data set\n",
      "    '''\n",
      "    start_states, controls, durations, end_states = [], [], [], [] \n",
      "    \n",
      "    with open('../data_output_50Hz.txt', 'r') as infile:\n",
      "        data = infile.readlines()\n",
      "        \n",
      "        idx, i = 0, 0\n",
      "        \n",
      "        for line in data:\n",
      "            # Stop at end of file\n",
      "            if line == '':\n",
      "                break\n",
      "                \n",
      "            # Reset and continue at Trajectory break\n",
      "            if len(line) == 1:\n",
      "                start_states.pop()\n",
      "                i = 0\n",
      "                continue\n",
      "                \n",
      "            # Split Values in line and append to individual lists\n",
      "            vals = line.split(',')\n",
      "            if i % 3 == 0:\n",
      "                start_states.append([float(val) for val in vals])\n",
      "                if i != 0:\n",
      "                    end_states.append([float(val) for val in vals])\n",
      "                    idx += 1\n",
      "            elif i % 3 == 1:\n",
      "                controls.append([float(val) for val in vals])\n",
      "            elif i % 3 == 2:\n",
      "                durations.append([float(val) for val in vals])\n",
      "                \n",
      "            i += 1\n",
      "            \n",
      "    X = np.concatenate((start_states, controls, durations), axis=1)\n",
      "    y = np.asarray(end_states, dtype=np.float32)\n",
      "    \n",
      "    X = normalize_data(X)\n",
      "    \n",
      "    # Shuffle the data around and split 3M training, ~750k validation\n",
      "    indices = np.random.permutation(X.shape[0])\n",
      "    training_idx, test_idx = indices[:3000000], indices[3000000:]\n",
      "    \n",
      "    train_X = X[training_idx,:]\n",
      "    train_y = y[training_idx,:]\n",
      "    \n",
      "    test_X = X[test_idx,:]\n",
      "    test_y = y[test_idx,:]\n",
      "                \n",
      "    return train_X, train_y, test_X, test_y\n",
      "\n",
      "\n",
      "def normalize_data(data):\n",
      "    '''\n",
      "    Normalize data to zero mean on [-1,1] interval for all dimensions\n",
      "    '''\n",
      "    X_mean = np.mean(data, axis = 0)\n",
      "    \n",
      "    # do not substract duration\n",
      "    X_mean[7] = 0\n",
      "    \n",
      "    # Mean Shift\n",
      "    data_t = data - X_mean\n",
      "    \n",
      "    # Find bounds, define desired bounds\n",
      "    X_min = np.min(data_t, axis=0)\n",
      "    X_max = np.max(data_t, axis=0)\n",
      "    desiredMin = -1;\n",
      "    desiredMax = 1;\n",
      "    \n",
      "    # Normalize \n",
      "    for i in range(0, 7):\n",
      "        data_t[:,i] = (data_t[:,i]-X_min[i])*(desiredMax-desiredMin)/(X_max[i]-X_min[i]) \\\n",
      "            + desiredMin\n",
      "\n",
      "    return data_t\n",
      "\n",
      "\n",
      "def save_data_as_hdf5(hdf5_data_filename, data, labels):\n",
      "    '''\n",
      "    HDF5 is one of the data formats Caffe accepts\n",
      "    '''\n",
      "    with h5py.File(hdf5_data_filename, 'w') as f:\n",
      "        f['data'] = data.astype(np.float32)\n",
      "        f['label'] = labels.astype(np.float32)\n",
      "\n",
      "\n",
      "def train(solver_prototxt_filename):\n",
      "    '''\n",
      "    Train the ANN\n",
      "    '''\n",
      "    caffe.set_mode_gpu()\n",
      "    solver = caffe.get_solver(solver_prototxt_filename)\n",
      "    solver.solve()\n",
      "\n",
      "\n",
      "def print_network_parameters(net):\n",
      "    '''\n",
      "    Print the parameters of the network\n",
      "    '''\n",
      "    print(net)\n",
      "    print('net.inputs: {0}'.format(net.inputs))\n",
      "    print('net.outputs: {0}'.format(net.outputs))\n",
      "    print('net.blobs: {0}'.format(net.blobs))\n",
      "    print('net.params: {0}'.format(net.params))    \n",
      "\n",
      "def get_predicted_output(deploy_prototxt_filename, caffemodel_filename, input, net = None):\n",
      "    '''\n",
      "    Get the predicted output, i.e. perform a forward pass\n",
      "    '''\n",
      "    if net is None:\n",
      "        net = caffe.Net(deploy_prototxt_filename,caffemodel_filename, caffe.TEST)\n",
      "    \n",
      "    print \"Input: \"\n",
      "    print input \n",
      "    out = net.forward(data=input)\n",
      "    return out[net.outputs[0]]\n",
      "\n",
      "\n",
      "import google.protobuf \n",
      "def print_network(prototxt_filename, caffemodel_filename):\n",
      "    '''\n",
      "    Draw the ANN architecture\n",
      "    '''\n",
      "    _net = caffe.proto.caffe_pb2.NetParameter()\n",
      "    f = open(prototxt_filename)\n",
      "    google.protobuf.text_format.Merge(f.read(), _net)\n",
      "    caffe.draw.draw_net_to_file(_net, prototxt_filename + '.png' )\n",
      "    print('Draw ANN done!')\n",
      "\n",
      "\n",
      "def print_network_weights(prototxt_filename, caffemodel_filename):\n",
      "    '''\n",
      "    For each ANN layer, print weight heatmap and weight histogram \n",
      "    '''\n",
      "    net = caffe.Net(prototxt_filename,caffemodel_filename, caffe.TEST)\n",
      "    for layer_name in net.params: \n",
      "        # weights heatmap \n",
      "        arr = net.params[layer_name][0].data\n",
      "        plt.clf()\n",
      "        fig = plt.figure(figsize=(10,10))\n",
      "        ax = fig.add_subplot(111)\n",
      "        cax = ax.matshow(arr, interpolation='none')\n",
      "        fig.colorbar(cax, orientation=\"horizontal\")\n",
      "        plt.savefig('{0}_weights_{1}.png'.format(caffemodel_filename, layer_name), dpi=100, format='png', bbox_inches='tight') # use format='svg' or 'pdf' for vectorial pictures\n",
      "        plt.close()\n",
      "\n",
      "        # weights histogram  \n",
      "        plt.clf()\n",
      "        plt.hist(arr.tolist(), bins=20)\n",
      "        plt.savefig('{0}_weights_hist_{1}.png'.format(caffemodel_filename, layer_name), dpi=100, format='png', bbox_inches='tight') # use format='svg' or 'pdf' for vectorial pictures\n",
      "        plt.close()\n",
      "\n",
      "\n",
      "def get_predicted_outputs(deploy_prototxt_filename, caffemodel_filename, inputs):\n",
      "    '''\n",
      "    Get several predicted outputs\n",
      "    '''\n",
      "    outputs = []\n",
      "    net = caffe.Net(deploy_prototxt_filename,caffemodel_filename, caffe.TRAIN)\n",
      "    outputs.append(copy.deepcopy(get_predicted_output(deploy_prototxt_filename, caffemodel_filename, inputs, net)))\n",
      "    return outputs    \n",
      "\n",
      "\n",
      "def get_accuracy(true_outputs, predicted_outputs):\n",
      "    '''\n",
      "\n",
      "    '''\n",
      "    number_of_samples = true_outputs.shape[0]\n",
      "    number_of_outputs = true_outputs.shape[1]\n",
      "    threshold = 0.0 # 0 if SigmoidCrossEntropyLoss ; 0.5 if EuclideanLoss\n",
      "    for output_number in range(number_of_outputs):\n",
      "        predicted_output_binary = []\n",
      "        for sample_number in range(number_of_samples):\n",
      "            #print(predicted_outputs)\n",
      "            #print(predicted_outputs[sample_number][output_number])            \n",
      "            if predicted_outputs[sample_number][0][output_number] < threshold:\n",
      "                predicted_output = 0\n",
      "            else:\n",
      "                predicted_output = 1\n",
      "            predicted_output_binary.append(predicted_output)\n",
      "\n",
      "        print('accuracy: {0}'.format(sklearn.metrics.accuracy_score(true_outputs[:, output_number], predicted_output_binary)))\n",
      "        print(sklearn.metrics.confusion_matrix(true_outputs[:, output_number], predicted_output_binary))\n",
      "\n",
      "\n",
      "def training(model_iter):\n",
      "    '''\n",
      "    Performs Training of the specified network and outputs PNG images\n",
      "    showing resulting learned weights and histograms of weights\n",
      "    '''\n",
      "    # Set parameters\n",
      "    solver_prototxt_filename = 'toycar_solver.prototxt'\n",
      "    train_test_prototxt_filename = 'toycar_2fc_hdf5.prototxt'\n",
      "    caffemodel_filename = '2fc_iter_' + str(model_iter + 1) + '.caffemodel' \n",
      "\n",
      "    # Train network\n",
      "    train(solver_prototxt_filename)\n",
      "\n",
      "    # Print network\n",
      "    print_network(train_test_prototxt_filename, caffemodel_filename)\n",
      "    print_network_weights(train_test_prototxt_filename, caffemodel_filename)\n",
      "\n",
      "\n",
      "def testing(deploy_prototxt_filename, caffemodel_filename, inputs, labels):\n",
      "    '''\n",
      "    Performs Testing of the specified network\n",
      "    '''    \n",
      "    # Compute performance metrics\n",
      "    outputs = get_predicted_outputs(deploy_prototxt_filename, caffemodel_filename, inputs)\n",
      "    \n",
      "    print 'predictions: '\n",
      "    print outputs[0]\n",
      "    \n",
      "    print 'ground truths: '\n",
      "    print labels\n",
      "    \n",
      "    return outputs[0]\n",
      "    \n",
      "    \n",
      "def euclidean_loss(pred, labels):\n",
      "    '''\n",
      "    Hand Calculate the Euclidean Loss to Compare with Model Output\n",
      "    '''\n",
      "    result = labels-pred\n",
      "    size = pred.shape[0]\n",
      "    \n",
      "    loss = np.sum(np.square(result), axis=1)\n",
      "    loss = np.sum(loss,axis=0)/(2*size)\n",
      "    \n",
      "    print \"Euclidean Loss: \", loss\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "save_data_as_hdf5('toycar_hdf5_data_random_norm11_train.hdf5', train_data, train_labels)\n",
      "save_data_as_hdf5('toycar_hdf5_data_random_norm11_test.hdf5', test_data, test_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_data, train_labels, test_data, test_labels = load_data()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "training(20000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Draw ANN done!\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pred = testing('toycar_2fc_deploy.prototxt', '2fc_iter_20001.caffemodel', \\\n",
      "               test_data[:1000,:], test_labels[:1000,:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Input: \n",
        "[[-0.05435242  0.1195259  -0.06501284 ...,  0.17664139  0.80321053  0.02      ]\n",
        " [-0.17756674  0.26720502  0.23875696 ...,  0.80791139  0.83252849  0.02      ]\n",
        " [ 0.07655204  0.38724243  0.47897971 ..., -0.93048528 -0.79375296  0.02      ]\n",
        " ..., \n",
        " [-0.66651934  0.27884553  0.31249015 ..., -0.08990942 -0.44698677  0.02      ]\n",
        " [-0.03177897  0.28902661  0.30727421 ..., -0.92060116  0.1163746   0.02      ]\n",
        " [-0.89317608  0.31935315  0.21769307 ...,  0.16639756  0.83882103  0.02      ]]\n",
        "predictions: \n",
        "[[ 1.47165012 -0.54705977 -0.72628057  1.35936189 -0.41245356]\n",
        " [ 0.76164889 -0.04335606 -0.0303295  -0.36298859 -0.24266   ]\n",
        " [ 2.21191955  0.43641868  0.46808109  0.79915768  0.19104682]\n",
        " ..., \n",
        " [-1.7192241  -0.03644708  0.10925958 -0.52883959 -0.29381287]\n",
        " [ 1.59881973  0.0411846   0.0951983  -0.31067312  0.15575606]\n",
        " [-2.94364262  0.15871605 -0.11993392 -0.87772936  0.1081139 ]]\n",
        "ground truths: \n",
        "[[ 1.49905443 -0.60389668 -0.73746616  1.34962273 -0.41802865]\n",
        " [ 0.82732171 -0.03008907 -0.06095383 -0.36584857 -0.25133502]\n",
        " [ 2.18184948  0.42945102  0.46499583  0.82053649  0.18059869]\n",
        " ..., \n",
        " [-1.73996532  0.01232277  0.10127047 -0.54398429 -0.28292781]\n",
        " [ 1.59330726  0.05125056  0.08589913 -0.30598366  0.16281195]\n",
        " [-2.93616176  0.16809563 -0.11067808 -0.90527856  0.10272782]]\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "euclidean_loss(pred, test_labels[:1000,:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Euclidean Loss:  0.000722401559353\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result = pred - test_labels[:1000,:]\n",
      "print np.sqrt(np.sum(np.square(result), axis=1)).shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1000,)\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}