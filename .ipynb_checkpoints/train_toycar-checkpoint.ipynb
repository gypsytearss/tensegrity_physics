{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Requirements:\n",
    " - Caffe (script to install Caffe and pycaffe on a new Ubuntu 14.04 LTS x64 or Ubuntu 14.10 x64. \n",
    "   CPU only, multi-threaded Caffe. http://stackoverflow.com/a/31396229/395857)\n",
    " - sudo pip install pydot\n",
    " - sudo apt-get install -y graphviz\n",
    "\n",
    "Interesting resources on Caffe:\n",
    " - https://github.com/BVLC/caffe/tree/master/examples\n",
    " - http://nbviewer.ipython.org/github/joyofdata/joyofdata-articles\\\n",
    " /blob/master/deeplearning-with-caffe/\\\n",
    " Neural-Networks-with-Caffe-on-the-GPU.ipynb\n",
    "'''\n",
    "\n",
    "import subprocess\n",
    "import platform\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# from sklearn.datasets import load_iris\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import caffe\n",
    "import caffe.draw\n",
    "import google.protobuf \n",
    "\n",
    "# Globals\n",
    "train_data, train_labels, test_data, test_labels = [], [], [], []\n",
    "\n",
    "def load_data():\n",
    "    '''\n",
    "    Load Sample for Forward Pass from Toy Car Data set\n",
    "    '''\n",
    "    start_states, controls, durations, end_states = [], [], [], []\n",
    "\n",
    "    with open('data_output_50Hz.txt', 'r') as infile:\n",
    "        data = infile.readlines()\n",
    "\n",
    "        idx, i = 0, 0\n",
    "\n",
    "        for line in data:\n",
    "            # Stop at end of file\n",
    "            if line == '':\n",
    "                break\n",
    "\n",
    "            # Reset and continue at Trajectory break\n",
    "            if len(line) == 1:\n",
    "                start_states.pop()\n",
    "                i = 0\n",
    "                if idx > 1000000:\n",
    "                    break\n",
    "                continue\n",
    "                \n",
    "            # Split Values in line and append to individual lists\n",
    "            vals = line.split(',')\n",
    "            if i % 3 == 0:\n",
    "                start_states.append([float(val) for val in vals])\n",
    "                if i != 0:\n",
    "                    end_states.append([float(val) for val in vals])\n",
    "                    idx += 1\n",
    "            elif i % 3 == 1:\n",
    "                controls.append([float(val) for val in vals])\n",
    "            elif i % 3 == 2:\n",
    "                durations.append([float(val) for val in vals])\n",
    "                \n",
    "            i += 1\n",
    "            \n",
    "    X = np.concatenate((start_states, controls, durations), axis=1)\n",
    "    start_states = np.asarray(start_states, dtype=np.float32)\n",
    "    end_states = np.asarray(end_states, dtype=np.float32)\n",
    "    \n",
    "    X, meanx, minx, maxx = normalize_data(X)\n",
    "    y = normalize_labels(start_states, end_states)\n",
    "\n",
    "    # Shuffle the data around and split 3M training, ~750k validation\n",
    "    indices = np.random.permutation(X.shape[0])\n",
    "    training_idx = indices[:900000]\n",
    "    \n",
    "    train_X = X[training_idx, :]\n",
    "    train_y = y[training_idx, :]\n",
    "\n",
    "    test_X = X[900000:1000000, :]\n",
    "    test_y = y[900000:1000000, :]\n",
    "\n",
    "    return train_X, train_y, test_X, test_y, meanx, minx, maxx\n",
    "\n",
    "\n",
    "def write_binaryproto(data, string):\n",
    "    blob = caffe.proto.caffe_pb2.BlobProto()\n",
    "    blob.channels = data.shape[0]\n",
    "    blob.data.extend(data.astype(float).flat)\n",
    "    binaryproto_file = open('toycar_' + string + '.binaryproto', 'wb')\n",
    "    binaryproto_file.write(blob.SerializeToString())\n",
    "    binaryproto_file.close()\n",
    "\n",
    "\n",
    "def save_binaryproto(data, ftype='mean'):\n",
    "    '''\n",
    "    Take the mean values of the raw data and store them as binaryproto type\n",
    "    In order to use them later for deploy normalization\n",
    "    '''\n",
    "    # Convert to 32bit float\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "\n",
    "    # Set project home dir \n",
    "    PROJECT_HOME = os.path.abspath('.')\n",
    "\n",
    "    # Initialize blob to store serialized means\n",
    "    blob = caffe.proto.caffe_pb2.BlobProto()\n",
    "\n",
    "    # Custom dimensions for blob for this project\n",
    "    blob.num = 1\n",
    "    blob.channels = data.shape[0]\n",
    "    blob.height = 1\n",
    "    blob.width = 1\n",
    "    \n",
    "    # Reshape data and copy into blob\\n\",\n",
    "    blob.data.extend(data.astype(float).flat)\n",
    "    \n",
    "    # Write file\n",
    "    binaryproto_file = open(PROJECT_HOME + '/toycar_' + ftype + '.binaryproto', 'wb')\n",
    "    binaryproto_file.write(blob.SerializeToString())\n",
    "    binaryproto_file.close()\n",
    "\n",
    "\n",
    "\n",
    "def normalize_labels(start_states, end_states):\n",
    "    '''\n",
    "    Normalize end states such that positional coordinates e.g. (x,y)\n",
    "    are now represented by delta_(x,y)\n",
    "    '''\n",
    "    y = end_states\n",
    "    y[:, 0] = end_states[:, 0] - start_states[:, 0]\n",
    "    y[:, 1] = end_states[:, 1] - start_states[:, 1]\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def unnormalize_data(data, meanx, minx, maxx):\n",
    "    desired_min = -1\n",
    "    desired_max = 1\n",
    "    desired_rng = desired_max - desired_min\n",
    "    \n",
    "    data = data - desired_min\n",
    "    for i in range(0, data.shape[0]):\n",
    "        data[:, i] = data[:, i] * (maxx[i] - minx[i]) \\\n",
    "                    / desired_rng + minx[i] + meanx[i]\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def normalize_data(data):\n",
    "    '''\n",
    "    Normalize data to zero mean on [-1,1] interval for all dimensions\n",
    "    '''\n",
    "    X_mean = np.mean(data, axis=0)\n",
    "    \n",
    "    # do not substract duration\n",
    "    X_mean[7] = 0\n",
    "    \n",
    "    # Mean Shift\n",
    "    data_t = data - X_mean\n",
    "    \n",
    "    # Find bounds, define desired bounds\n",
    "    X_min = np.min(data_t, axis=0)\n",
    "    X_max = np.max(data_t, axis=0)\n",
    "\n",
    "    # Write 'em Out\n",
    "    save_binaryproto(X_mean, ftype=\"mean\")\n",
    "    save_binaryproto(X_min, ftype=\"min\")\n",
    "    save_binaryproto(X_max, ftype=\"max\")\n",
    "\n",
    "    desiredMin = -1\n",
    "    desiredMax = 1\n",
    "    \n",
    "    # Normalize \n",
    "    for i in range(0, 7):\n",
    "        data_t[:, i] = (data_t[:, i] - X_min[i]) * (desiredMax - desiredMin)\\\n",
    "            / (X_max[i] - X_min[i]) + desiredMin\n",
    "\n",
    "    return data_t, X_mean, X_min, X_max\n",
    "\n",
    "\n",
    "def normalize_test_data(data, meanx, minx, maxx):\n",
    "    '''\n",
    "    Normalize data to zero mean on [-1,1] interval for all dimensions\n",
    "    '''\n",
    "    \n",
    "    # Mean Shift\n",
    "    data_t = data - meanx\n",
    "    \n",
    "    desiredMin = -1\n",
    "    desiredMax = 1\n",
    "    \n",
    "    # Normalize \n",
    "    for i in range(0, data.shape[0]):\n",
    "        data_t[:, i] = (data_t[:, i] - minx[i]) * (desiredMax - desiredMin)\\\n",
    "            / (maxx[i] - minx[i]) + desiredMin\n",
    "\n",
    "    return data_t\n",
    "\n",
    "\n",
    "def save_data_as_hdf5(hdf5_data_filename, data, labels):\n",
    "    '''\n",
    "    HDF5 is one of the data formats Caffe accepts\n",
    "    '''\n",
    "    with h5py.File(hdf5_data_filename, 'w') as f:\n",
    "        f['data'] = data.astype(np.float32)\n",
    "        f['label'] = labels.astype(np.float32)\n",
    "\n",
    "\n",
    "def train(solver_prototxt_filename):\n",
    "    '''\n",
    "    Train the ANN\n",
    "    '''\n",
    "    # caffe.set_mode_gpu()\n",
    "    solver = caffe.get_solver(solver_prototxt_filename)\n",
    "    solver.solve()\n",
    "\n",
    "\n",
    "def print_network_parameters(net):\n",
    "    '''\n",
    "    Print the parameters of the network\n",
    "    '''\n",
    "    print(net)\n",
    "    print('net.inputs: {0}'.format(net.inputs))\n",
    "    print('net.outputs: {0}'.format(net.outputs))\n",
    "    print('net.blobs: {0}'.format(net.blobs))\n",
    "    print('net.params: {0}'.format(net.params))    \n",
    "\n",
    "\n",
    "def get_predicted_output(deploy_prototxt_filename, \n",
    "                         caffemodel_filename, input, net=None):\n",
    "    '''\n",
    "    Get the predicted output, i.e. perform a forward pass\n",
    "    '''\n",
    "    if net is None:\n",
    "        net = caffe.Net(deploy_prototxt_filename, \n",
    "                        caffemodel_filename, caffe.TEST)\n",
    "    \n",
    "#     print \"Input: \"\n",
    "#     print input \n",
    "    out = net.forward(data=input)\n",
    "    return out[net.outputs[0]]\n",
    "\n",
    "\n",
    "def print_network(prototxt_filename, caffemodel_filename):\n",
    "    '''\n",
    "    Draw the ANN architecture\n",
    "    '''\n",
    "    _net = caffe.proto.caffe_pb2.NetParameter()\n",
    "    f = open(prototxt_filename)\n",
    "    google.protobuf.text_format.Merge(f.read(), _net)\n",
    "    caffe.draw.draw_net_to_file(_net, prototxt_filename + '.png')\n",
    "    print('Draw ANN done!')\n",
    "\n",
    "\n",
    "def print_network_weights(prototxt_filename, caffemodel_filename):\n",
    "    '''\n",
    "    For each ANN layer, print weight heatmap and weight histogram \n",
    "    '''\n",
    "    net = caffe.Net(prototxt_filename, caffemodel_filename, caffe.TEST)\n",
    "    for layer_name in net.params: \n",
    "        # weights heatmap \n",
    "        arr = net.params[layer_name][0].data\n",
    "        plt.clf()\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(arr, interpolation='none')\n",
    "        fig.colorbar(cax, orientation=\"horizontal\")\n",
    "        plt.savefig('{0}_weights_{1}.png'.format(caffemodel_filename, \n",
    "                                                 layer_name), \n",
    "                    dpi=100, format='png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # weights histogram  \n",
    "        plt.clf()\n",
    "        plt.hist(arr.tolist(), bins=20)\n",
    "        # savefig: use format='svg' or 'pdf' for vectorial pictures\n",
    "        plt.savefig('{0}_weights_hist_{1}.png'.format(caffemodel_filename, \n",
    "                                                      layer_name), dpi=100, \n",
    "                    format='png', \n",
    "                    bbox_inches='tight')  \n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def get_predicted_outputs(deploy_prototxt_filename, \n",
    "                          caffemodel_filename, inputs):\n",
    "    '''\n",
    "    Get several predicted outputs\n",
    "    '''\n",
    "    outputs = []\n",
    "    net = caffe.Net(deploy_prototxt_filename, caffemodel_filename, caffe.TRAIN)\n",
    "    outputs.append(copy.deepcopy(get_predicted_output(deploy_prototxt_filename, \n",
    "                                                      caffemodel_filename, \n",
    "                                                      inputs, net)))\n",
    "    return outputs    \n",
    "\n",
    "\n",
    "def get_accuracy(true_outputs, predicted_outputs):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    number_of_samples = true_outputs.shape[0]\n",
    "    number_of_outputs = true_outputs.shape[1]\n",
    "    threshold = 0.0  # 0 if SigmoidCrossEntropyLoss ; 0.5 if EuclideanLoss\n",
    "    for output_number in range(number_of_outputs):\n",
    "        predicted_output_binary = []\n",
    "        for sample_number in range(number_of_samples):\n",
    "            # print(predicted_outputs)\n",
    "            # print(predicted_outputs[sample_number][output_number])            \n",
    "            if predicted_outputs[sample_number][0][output_number] < threshold:\n",
    "                predicted_output = 0\n",
    "            else:\n",
    "                predicted_output = 1\n",
    "            predicted_output_binary.append(predicted_output)\n",
    "\n",
    "        print('accuracy: {0}'.format(sklearn.metrics.accuracy_score(\n",
    "                                     true_outputs[:, output_number], \n",
    "                                     predicted_output_binary)))\n",
    "        print(sklearn.metrics.confusion_matrix(true_outputs[:, output_number], \n",
    "                                               predicted_output_binary))\n",
    "\n",
    "\n",
    "def training(model_iter):\n",
    "    '''\n",
    "    Performs Training of the specified network and outputs PNG images\n",
    "    showing resulting learned weights and histograms of weights\n",
    "    '''\n",
    "    # Set parameters\n",
    "    solver_prototxt_filename = 'toycar_solver.prototxt'\n",
    "    train_test_prototxt_filename = 'toycar_2fc_hdf5.prototxt'\n",
    "    caffemodel_filename = '2fc_iter_' + str(model_iter) + '.caffemodel' \n",
    "\n",
    "    # Train network\n",
    "    train(solver_prototxt_filename)\n",
    "\n",
    "    # Print network\n",
    "    print_network(train_test_prototxt_filename, caffemodel_filename)\n",
    "    print_network_weights(train_test_prototxt_filename, caffemodel_filename)\n",
    "\n",
    "\n",
    "def testing(deploy_prototxt_filename, caffemodel_filename, inputs, labels):\n",
    "    '''\n",
    "    Performs Testing of the specified network\n",
    "    '''    \n",
    "    # Compute performance metrics\n",
    "    outputs = get_predicted_outputs(deploy_prototxt_filename, \n",
    "                                    caffemodel_filename, inputs)\n",
    "    \n",
    "#     print 'predictions: '\n",
    "#     print outputs[0]\n",
    "    \n",
    "#     print 'ground truths: '\n",
    "#     print labels\n",
    "    \n",
    "    return outputs[0]\n",
    "    \n",
    "    \n",
    "def euclidean_loss(pred, labels):\n",
    "    '''\n",
    "    Hand Calculate the Euclidean Loss to Compare with Model Output\n",
    "    '''\n",
    "    result = labels-pred\n",
    "    size = pred.shape[0]\n",
    "    \n",
    "    loss = np.sum(np.square(result), axis=1)\n",
    "    loss = np.sum(loss, axis=0) / (2 * size)\n",
    "    \n",
    "    print \"Euclidean Loss: \", loss\n",
    "\n",
    "\n",
    "def main(arg=\"x\"):\n",
    "\n",
    "    if arg.lower() == \"train\":\n",
    "        train_data, train_labels, test_data, test_labels = load_data()\n",
    "\n",
    "        # save_data_as_hdf5('toycar_hdf5_data_random_norm11_train.hdf5', \n",
    "        #                   train_data, train_labels)\n",
    "        # save_data_as_hdf5('toycar_hdf5_data_random_norm11_test.hdf5', \n",
    "        #                   test_data, test_labels)\n",
    "        \n",
    "        solver_name = \"toycar_solver.prototxt\"\n",
    "        training(20000)\n",
    "\n",
    "    elif arg.lower() == \"test\":\n",
    "        \n",
    "        train_data, train_labels, test_data, test_labels, meanx, minx, maxx = load_data()\n",
    "\n",
    "        pred = testing('toycar_2fc_deploy.prototxt', \n",
    "               '2fc_iter_100000.caffemodel', \n",
    "               test_data[0:1, :], test_labels[0:1, :])\n",
    "        print \"Input: \", test_data[0:1, :]\n",
    "#             pred[0, 0] = pred[0, 0] + test_data[0, 0]\n",
    "#             pred[0, 1] = pred[0, 1] + test_data[0, 1]\n",
    "        print \"Pred: \", pred\n",
    "        print \"Label: \", test_labels[0:1, :]\n",
    "        print \"Loss: \", euclidean_loss(pred, test_labels[0:1, :])\n",
    "\n",
    "\n",
    "        for i in range(1, 100):\n",
    "            \n",
    "#             print pred.shape\n",
    "#             print test_data[i:i+1,5:8].shape\n",
    "            \n",
    "            pred[0, 0] = pred[0, 0] + test_data[i-1, 0]\n",
    "            pred[0, 1] = pred[0, 1] + test_data[i-1, 1]\n",
    "            \n",
    "            pred = normalize_test_data(pred, meanx[0:5], minx[0:5], maxx[0:5])\n",
    "            \n",
    "            inpt = np.asarray([np.concatenate((pred[0,:], test_data[i, 5:8]), axis=0)])\n",
    "            \n",
    "            print \"Test Data: \", test_data[i,:]\n",
    "            print \"Input: \", inpt\n",
    "            pred = testing('toycar_2fc_deploy.prototxt', \n",
    "                           '2fc_iter_100000.caffemodel', \n",
    "                           inpt,  test_labels[i:i+1, :])\n",
    "            \n",
    "            print \"Pred: \", pred\n",
    "            print \"Label: \", test_labels[i:i+1, :]\n",
    "            print \"Loss: \", euclidean_loss(pred, test_labels[i:i+1, :])\n",
    "    else:\n",
    "        train_data, train_labels, test_data, test_labels = load_data()\n",
    "\n",
    "\n",
    "    # result = pred - test_labels[:1000, :]\n",
    "    # print np.sqrt(np.sum(np.square(result), axis=1)).shape\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [[-0.52753878 -0.09802863  0.13742372 -0.54690999  0.16745029 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.00133245 -0.0015051   0.13568482 -0.09937215  0.09792733]]\n",
      "Label:  [[-0.00196064 -0.00026777  0.13562027 -0.10375793  0.09954461]]\n",
      "Loss:  Euclidean Loss:  1.18902407849e-05\n",
      "None\n",
      "Test Data:  [-0.52800329 -0.09813368  0.13728903 -0.55064514  0.17081498 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.42742072 -0.10758291  0.10201812 -0.3736182   0.08950966 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[ 0.0052745  -0.00043951  0.0869709   0.24283138  0.05460171]]\n",
      "Label:  [[-0.00210685 -0.00028731  0.13540609 -0.11119144  0.10154913]]\n",
      "Loss:  Euclidean Loss:  0.0649683475494\n",
      "None\n",
      "Test Data:  [-0.52850244 -0.09824639  0.13714132 -0.55438028  0.17417968 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.42596547 -0.10662237  0.05330421 -0.03141467  0.04618403 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[ 0.01705209  0.00148455  0.00539215  0.90187484  0.02689805]]\n",
      "Label:  [[-0.002253   -0.00030673  0.13517247 -0.11862496  0.10355365]]\n",
      "Loss:  Euclidean Loss:  0.532257378101\n",
      "None\n",
      "Test Data:  [-0.52903622 -0.09836672  0.1369802  -0.55811543  0.17754437 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.42329339 -0.10481103 -0.02827454  0.62762879  0.01848037 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[ 0.04158458 -0.01011214 -0.10348033  2.2792418   0.00761602]]\n",
      "Label:  [[-0.00239915 -0.00032603  0.13491879 -0.12605846  0.10555818]]\n",
      "Loss:  Euclidean Loss:  2.92696285248\n",
      "None\n",
      "Test Data:  [-0.52960462 -0.09849463  0.13680525 -0.56185058  0.18090906 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[ -4.17607652e-01  -1.16528055e-01  -1.37147025e-01   2.00499575e+00\n",
      "   -8.01649711e-04  -3.71710775e-01   5.01079724e-01   2.00000000e-02]]\n",
      "Pred:  [[  9.91649777e-02  -3.10501084e-03  -1.92931741e-01   5.02355337e+00\n",
      "   -8.62273723e-02]]\n",
      "Label:  [[-0.00254524 -0.0003452   0.13464448 -0.13349198  0.1075627 ]]\n",
      "Loss:  Euclidean Loss:  13.3751649857\n",
      "None\n",
      "Test Data:  [-0.53020763 -0.09863006  0.13661607 -0.56558573  0.18427376 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.40410045 -0.10964883 -0.22659844  4.74930732 -0.09464505 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[  1.99522927e-01   3.02840769e-03  -1.52918458e-01   1.05845528e+01\n",
      "   -2.77185708e-01]]\n",
      "Label:  [[-0.00269121 -0.00036423  0.13434893 -0.14092548  0.10956722]]\n",
      "Loss:  Euclidean Loss:  57.6544418335\n",
      "None\n",
      "Test Data:  [-0.53084522 -0.09877295  0.13641224 -0.56932087  0.18763845 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[ -0.38046666  -0.10365084  -0.18658515  10.31030671  -0.28560338\n",
      "   -0.37171077   0.50107972   0.02      ]]\n",
      "Pred:  [[  0.40578458   0.03144883   0.11597473  21.97484398  -0.63510501]]\n",
      "Label:  [[-0.00283718 -0.0003831   0.13403155 -0.14835899  0.11157174]]\n",
      "Loss:  Euclidean Loss:  245.080978394\n",
      "None\n",
      "Test Data:  [-0.5315174  -0.09892324  0.13619336 -0.57305602  0.19100315 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[ -3.31750508e-01  -7.53733051e-02   8.23080382e-02   2.17005979e+01\n",
      "   -6.43522688e-01  -3.71710775e-01   5.01079724e-01   2.00000000e-02]]\n",
      "Pred:  [[  0.83302313   0.10881094   0.85674554  45.37226486  -1.33651555]]\n",
      "Label:  [[-0.00298303 -0.0004018   0.13369176 -0.1557925   0.11357626]]\n",
      "Loss:  Euclidean Loss:  1038.07019043\n",
      "None\n",
      "Test Data:  [-0.53222415 -0.09908088  0.13595903 -0.57679117  0.19436784 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[ -2.30689030e-01   1.83850612e-03   8.23078848e-01   4.50980188e+01\n",
      "   -1.34493322e+00  -3.71710775e-01   5.01079724e-01   2.00000000e-02]]\n",
      "Pred:  [[  1.70655084   0.28132254   2.58140969  93.52150726  -2.72184634]]\n",
      "Label:  [[-0.00312895 -0.00042034  0.13332896 -0.16322601  0.11558078]]\n",
      "Loss:  Euclidean Loss:  4396.9375\n",
      "None\n",
      "Test Data:  [-0.53296544 -0.09924579  0.13570882 -0.58052632  0.19773254 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[ -2.39015684e-02   1.74192475e-01   2.54774300e+00   9.32472612e+01\n",
      "   -2.73026402e+00  -3.71710775e-01   5.01079724e-01   2.00000000e-02]]\n",
      "Pred:  [[   3.49440455    0.64450961    6.33451605  192.67417908   -5.48175478]]\n",
      "Label:  [[-0.00327468 -0.00043868  0.13294257 -0.17065951  0.1175853 ]]\n",
      "Loss:  Euclidean Loss:  18635.7949219\n",
      "None\n",
      "Test Data:  [-0.53374128 -0.09941789  0.13544235 -0.58426147  0.20109723 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[  3.99498505e-01   5.37214639e-01   6.30084935e+00   1.92399933e+02\n",
      "   -5.49017245e+00  -3.71710775e-01   5.01079724e-01   2.00000000e-02]]\n",
      "Pred:  [[   7.18190861    1.39918256   14.35826588  396.87884521  -11.13584995]]\n",
      "Label:  [[-0.00342041 -0.00045682  0.132532   -0.17809303  0.11958983]]\n",
      "Loss:  Euclidean Loss:  79018.4296875\n",
      "None\n",
      "Test Data:  [-0.53455164 -0.09959711  0.13515919 -0.58799661  0.20446192 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[  1.27295271e+00   1.29171544e+00   1.43245992e+01   3.96604599e+02\n",
      "   -1.11442676e+01  -3.71710775e-01   5.01079724e-01   2.00000000e-02]]\n",
      "Pred:  [[  14.77725887    2.96724248   31.51949501  817.3661499   -22.90990448]]\n",
      "Label:  [[-0.00356609 -0.00047476  0.13209665 -0.18552653  0.12159435]]\n",
      "Loss:  Euclidean Loss:  335066.8125\n",
      "None\n",
      "Test Data:  [-0.53539652 -0.09978336  0.13485895 -0.59173176  0.20782662 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[  3.07224004e+00   2.85959608e+00   3.14858283e+01   8.17091904e+02\n",
      "   -2.29183222e+01  -3.71710775e-01   5.01079724e-01   2.00000000e-02]]\n",
      "Pred:  [[   30.42207527     6.22860813    67.65691376  1683.39929199\n",
      "    -47.23500061]]\n",
      "Label:  [[-0.0037117  -0.00049245  0.13163593 -0.19296005  0.12359887]]\n",
      "Loss:  Euclidean Loss:  1421125.0\n",
      "None\n",
      "Test Data:  [-0.53627589 -0.09997656  0.13454121 -0.59546691  0.21119131 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[  6.77858687e+00   6.12077552e+00   6.76232471e+01   1.68312505e+03\n",
      "   -4.72434183e+01  -3.71710775e-01   5.01079724e-01   2.00000000e-02]]\n",
      "Pred:  [[   62.64271164    12.98681164   143.11871338  3467.30981445\n",
      "    -97.40246582]]\n",
      "Label:  [[-0.00385731 -0.00050991  0.13114925 -0.20039356  0.12560339]]\n",
      "Loss:  Euclidean Loss:  6028839.0\n",
      "None\n",
      "Test Data:  [-0.53718975 -0.1001766   0.13420558 -0.59920206  0.21455601 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[  1.44120441e+01   1.28787859e+01   1.43085047e+02   3.46703557e+03\n",
      "   -9.74108835e+01  -3.71710775e-01   5.01079724e-01   2.00000000e-02]]\n",
      "Pred:  [[  129.01582336    26.96153641   299.89743042  7142.20800781\n",
      "   -200.79336548]]\n",
      "Label:  [[-0.00400275 -0.00052712  0.13063602 -0.20782706  0.12760791]]\n",
      "Loss:  Euclidean Loss:  25580854.0\n",
      "None\n",
      "Test Data:  [-0.53813808 -0.1003834   0.13385163 -0.60293721  0.2179207  -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[  3.01368496e+01   2.68533114e+01   2.99863764e+02   7.14193376e+03\n",
      "   -2.00801783e+02  -3.71710775e-01   5.01079724e-01   2.00000000e-02]]\n",
      "Pred:  [[   265.74908447     55.81948471    624.60223389  14712.95019531\n",
      "    -413.82919312]]\n",
      "Label:  [[-0.00414819 -0.00054406  0.13009568 -0.21526058  0.12961243]]\n",
      "Loss:  Euclidean Loss:  108556152.0\n",
      "None\n",
      "Test Data:  [-0.53912087 -0.10059684  0.13347897 -0.60667235  0.22128539 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[  6.25312660e+01   5.57110518e+01   6.24568567e+02   1.47126759e+04\n",
      "   -4.13837611e+02  -3.71710775e-01   5.01079724e-01   2.00000000e-02]]\n",
      "Pred:  [[   547.44000244    115.36077881   1295.79003906  30310.07617188\n",
      "    -852.75830078]]\n",
      "Label:  [[-0.00429356 -0.00056072  0.12952758 -0.22269408  0.13161695]]\n",
      "Loss:  Euclidean Loss:  460716704.0\n",
      "None\n",
      "Test Data:  [-0.5401381  -0.10081682  0.13308719 -0.6104075   0.22465009 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[  1.29268835e+02   1.15252136e+02   1.29575637e+03   3.03098019e+04\n",
      "   -8.52766718e+02  -3.71710775e-01   5.01079724e-01   2.00000000e-02]]\n",
      "Pred:  [[  1127.7800293     238.14431763   2681.49633789  62443.6484375\n",
      "   -1757.08740234]]\n",
      "Label:  [[-0.004439   -0.00057708  0.12893119 -0.23012759  0.13362147]]\n",
      "Loss:  Euclidean Loss:  1955422080.0\n",
      "None\n",
      "Test Data:  [-0.54118976 -0.10104322  0.13267589 -0.61414265  0.22801478 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[  2.66761899e+02   2.38035454e+02   2.68146267e+03   6.24433742e+04\n",
      "   -1.75709582e+03  -3.71710775e-01   5.01079724e-01   2.00000000e-02]]\n",
      "Pred:  [[   2323.41503906     491.2593689     5540.18115234  128646.7890625\n",
      "    -3620.27978516]]\n",
      "Label:  [[-0.00458419 -0.00059313  0.1283059  -0.23756111  0.13562599]]\n",
      "Loss:  Euclidean Loss:  8299747840.0\n",
      "None\n",
      "Test Data:  [-0.54227584 -0.10127591  0.13224465 -0.6178778   0.23137948 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[  5.50029736e+02   4.91150276e+02   5.54014749e+03   1.28646515e+05\n",
      "   -3.62028820e+03  -3.71710775e-01   5.01079724e-01   2.00000000e-02]]\n",
      "Pred:  [[   4786.72802734    1012.93731689   11434.74511719  265042.75\n",
      "    -7459.00439453]]\n",
      "Label:  [[-0.00472939 -0.00060885  0.12765111 -0.24499461  0.13763052]]\n",
      "Loss:  Euclidean Loss:  35229061120.0\n",
      "None\n",
      "Test Data:  [-0.54339632 -0.10151478  0.13179308 -0.62161295  0.23474417 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[  1.13363396e+03   1.01282801e+03   1.14347115e+04   2.65042476e+05\n",
      "   -7.45901281e+03  -3.71710775e-01   5.01079724e-01   2.00000000e-02]]\n",
      "Pred:  [[   9861.81054688    2087.99121094   23585.56445312  546055.75\n",
      "   -15367.8984375 ]]\n",
      "Label:  [[-0.00487447 -0.00062423  0.12696625 -0.25242811  0.13963504]]\n",
      "Loss:  Euclidean Loss:  149535621120.0\n",
      "None\n",
      "Test Data:  [-0.54455119 -0.10175967  0.13132076 -0.62534809  0.23810886 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[  2.33601497e+03   2.08788160e+03   2.35855308e+04   5.46055476e+05\n",
      "   -1.53679069e+04  -3.71710775e-01   5.01079724e-01   2.00000000e-02]]\n",
      "Pred:  [[   20317.88671875     4303.23193359    48627.94921875  1125021.25\n",
      "    -31662.51953125]]\n",
      "Label:  [[-0.00501966 -0.00063925  0.12625073 -0.25986162  0.14163956]]\n",
      "Loss:  Euclidean Loss:  634736017408.0\n",
      "None\n",
      "Test Data:  [-0.54574043 -0.10201046  0.1308273  -0.62908324  0.24147356 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[  4.81325278e+03   4.30312232e+03   4.86279156e+04   1.12502098e+06\n",
      "   -3.16625279e+04  -3.71710775e-01   5.01079724e-01   2.00000000e-02]]\n",
      "Pred:  [[   41860.34765625     8867.68261719   100233.265625    2317854.\n",
      "    -65234.28515625]]\n",
      "Label:  [[-0.00516462 -0.0006539   0.12550396 -0.26729515  0.14364408]]\n",
      "Loss:  Euclidean Loss:  2.69429086618e+12\n",
      "None\n",
      "Test Data:  [-0.5264437  -0.09778059  0.13787201 -0.46100159  0.09006232 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  9.91706064e+03   8.86757301e+03   1.00233232e+05   2.31785373e+06\n",
      "   -6.52342936e+04  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[   86243.9375        18272.32421875   206569.125       4775431.\n",
      "   -134401.5625    ]]\n",
      "Label:  [[ 0.00135124  0.00018559  0.13653767  0.05939361  0.05476179]]\n",
      "Loss:  Euclidean Loss:  1.14366233969e+13\n",
      "None\n",
      "Test Data:  [-0.52612357 -0.09770778  0.13792171 -0.46866567  0.09564466 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  2.04323614e+04   1.82722185e+04   2.06569091e+05   4.77543073e+06\n",
      "   -1.34401571e+05  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  177686.828125      37649.31640625   425669.90625     9838747.\n",
      "   -276906.5625    ]]\n",
      "Label:  [[ 0.00104922  0.00014418  0.13659716  0.04414092  0.05808748]]\n",
      "Loss:  Euclidean Loss:  4.85459062948e+13\n",
      "None\n",
      "Test Data:  [-0.52587498 -0.09765122  0.13796274 -0.47632975  0.101227   -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  4.20968754e+04   3.76492107e+04   4.25669873e+05   9.83874673e+06\n",
      "   -2.76906571e+05  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[   366085.59375     77572.5        877104.75     20270640.        -570508.1875 ]]\n",
      "Label:  [[  7.47382641e-04   1.02743506e-04   1.36642039e-01   2.88882405e-02\n",
      "    6.14131689e-02]]\n",
      "Loss:  Euclidean Loss:  2.06066843386e+14\n",
      "None\n",
      "Test Data:  [-0.52569792 -0.09761091  0.1379937  -0.48399383  0.10680935 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  8.67320353e+04   7.75723982e+04   8.77104716e+05   2.02706397e+07\n",
      "   -5.70508196e+05  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[   754241.4375    159826.875    1807221.25    41763360.      -1175413.    ]]\n",
      "Label:  [[  4.45544720e-04   6.12661242e-05   1.36670291e-01   1.36355562e-02\n",
      "    6.47388548e-02]]\n",
      "Loss:  Euclidean Loss:  8.74710220931e+14\n",
      "None\n",
      "Test Data:  [-0.52559236 -0.09758687  0.13801318 -0.49165792  0.11239169 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  1.78693357e+05   1.59826773e+05   1.80722122e+06   4.17633597e+07\n",
      "   -1.17541301e+06  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  1553955.     329296.5   3723570.   86044592.   -2421693. ]]\n",
      "Label:  [[  1.43945217e-04   1.97961926e-05   1.36679873e-01  -1.61712745e-03\n",
      "    6.80645406e-02]]\n",
      "Loss:  Euclidean Loss:  3.7129621833e+15\n",
      "None\n",
      "Test Data:  [-0.52555826 -0.09757911  0.13801979 -0.499322    0.11797403 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  3.68160325e+05   3.29296398e+05   3.72356997e+06   8.60445917e+07\n",
      "   -2.42169301e+06  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  3.20159650e+06   6.78455125e+05   7.67186100e+06   1.77276768e+08\n",
      "   -4.98939400e+06]]\n",
      "Label:  [[ -1.57594681e-04  -2.16662884e-05   1.36668772e-01  -1.68698113e-02\n",
      "    7.13902265e-02]]\n",
      "Loss:  Euclidean Loss:  1.57607572286e+16\n",
      "None\n",
      "Test Data:  [-0.52559559 -0.09758761  0.13801213 -0.50698608  0.12355637 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  7.58517139e+05   6.78454992e+05   7.67186097e+06   1.77276768e+08\n",
      "   -4.98939401e+06  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  6.59621650e+06   1.39782600e+06   1.58065480e+07   3.65241504e+08\n",
      "   -1.02796040e+07]]\n",
      "Label:  [[ -4.58955765e-04  -6.31138682e-05   1.36634961e-01  -3.21224965e-02\n",
      "    7.47159198e-02]]\n",
      "Loss:  Euclidean Loss:  6.69011722665e+16\n",
      "None\n",
      "Test Data:  [-0.52570432 -0.09761237  0.13798882 -0.51465016  0.12913871 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  1.56276554e+06   1.39782587e+06   1.58065480e+07   3.65241504e+08\n",
      "   -1.02796040e+07  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  1.35901160e+07   2.87994200e+06   3.25664520e+07   7.52503424e+08\n",
      "   -2.11789520e+07]]\n",
      "Label:  [[ -7.60138035e-04  -1.04501843e-04   1.36576414e-01  -4.73751798e-02\n",
      "    7.80416057e-02]]\n",
      "Loss:  Euclidean Loss:  2.83981742963e+17\n",
      "None\n",
      "Test Data:  [-0.52588442 -0.09765337  0.13794844 -0.52231425  0.13472105 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  3.21974986e+06   2.87994199e+06   3.25664520e+07   7.52503424e+08\n",
      "   -2.11789520e+07  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  2.79995840e+07   5.93353050e+06   6.70968240e+07   1.55037542e+09\n",
      "   -4.36347680e+07]]\n",
      "Label:  [[-0.00106126 -0.00014581  0.13649112 -0.06262786  0.08136729]]\n",
      "Loss:  Euclidean Loss:  1.20544462531e+18\n",
      "None\n",
      "Test Data:  [-0.52613585 -0.09771057  0.13788961 -0.52997833  0.14030339 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  6.63362018e+06   5.93353049e+06   6.70968240e+07   1.55037542e+09\n",
      "   -4.36347680e+07  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  5.76872720e+07   1.22248290e+07   1.38239648e+08   3.19422310e+09\n",
      "   -8.99003520e+07]]\n",
      "Label:  [[-0.00136214 -0.00018702  0.13637705 -0.07788055  0.08469298]]\n",
      "Loss:  Euclidean Loss:  5.1168654319e+18\n",
      "None\n",
      "Test Data:  [-0.52645857 -0.09778394  0.13781094 -0.53764241  0.14588573 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  1.36671838e+07   1.22248290e+07   1.38239648e+08   3.19422310e+09\n",
      "   -8.99003520e+07  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  1.18852504e+08   2.51867280e+07   2.84814688e+08   6.58102682e+09\n",
      "   -1.85220992e+08]]\n",
      "Label:  [[-0.00166291 -0.0002281   0.13623217 -0.09313323  0.08801866]]\n",
      "Loss:  Euclidean Loss:  2.17200495632e+19\n",
      "None\n",
      "Test Data:  [-0.52685255 -0.09787343  0.13771103 -0.54530649  0.15146807 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  2.81583611e+07   2.51867280e+07   2.84814688e+08   6.58102682e+09\n",
      "   -1.85220992e+08  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  2.44870688e+08   5.18920000e+07   5.86801792e+08   1.35588270e+10\n",
      "   -3.81608704e+08]]\n",
      "Label:  [[-0.0019635  -0.000269    0.13605449 -0.10838591  0.09134436]]\n",
      "Loss:  Euclidean Loss:  9.21972077864e+19\n",
      "None\n",
      "Test Data:  [-0.52731773 -0.09797896  0.13758848 -0.55297058  0.15705041 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  5.80144049e+07   5.18920000e+07   5.86801792e+08   1.35588270e+10\n",
      "   -3.81608704e+08  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  5.04504704e+08   1.06912720e+08   1.20898330e+09   2.79351194e+10\n",
      "   -7.86225152e+08]]\n",
      "Label:  [[-0.00226384 -0.00030973  0.13584195 -0.1236386   0.09467004]]\n",
      "Loss:  Euclidean Loss:  3.91358304019e+20\n",
      "None\n",
      "Test Data:  [-0.52785408 -0.09810047  0.13744191 -0.56063466  0.16263275 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  1.19526516e+08   1.06912720e+08   1.20898330e+09   2.79351194e+10\n",
      "   -7.86225152e+08  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  1.03942618e+09   2.20271184e+08   2.49086131e+09   5.75544566e+10\n",
      "   -1.61985126e+09]]\n",
      "Label:  [[-0.00256407 -0.0003502   0.13559256 -0.13889128  0.09799573]]\n",
      "Loss:  Euclidean Loss:  1.66123632062e+21\n",
      "None\n",
      "Test Data:  [-0.52846156 -0.09823786  0.13726992 -0.56829874  0.1682151  -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  2.46259329e+08   2.20271184e+08   2.49086131e+09   5.75544566e+10\n",
      "   -1.61985126e+09  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  2.14151974e+09   4.53822912e+08   5.13189888e+09   1.18578889e+11\n",
      "   -3.33736346e+09]]\n",
      "Label:  [[-0.00286412 -0.0003904   0.13530432 -0.15414396  0.10132141]]\n",
      "Loss:  Euclidean Loss:  7.05161019575e+21\n",
      "None\n",
      "Test Data:  [-0.52914012 -0.09839102  0.13707113 -0.57596282  0.17379744 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  5.07365725e+08   4.53822912e+08   5.13189888e+09   1.18578889e+11\n",
      "   -3.33736346e+09  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  4.41215232e+09   9.35007104e+08   1.05732055e+10   2.44306936e+11\n",
      "   -6.87594291e+09]]\n",
      "Label:  [[-0.00316393 -0.00043029  0.13497517 -0.16939665  0.1046471 ]]\n",
      "Loss:  Euclidean Loss:  2.99326457504e+22\n",
      "None\n",
      "Test Data:  [-0.52988972 -0.09855983  0.13684413 -0.58362691  0.17937978 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  1.04532067e+09   9.35007104e+08   1.05732055e+10   2.44306936e+11\n",
      "   -6.87594291e+09  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  9.09031424e+09   1.92638592e+09   2.17838756e+10   5.03343219e+11\n",
      "   -1.41664338e+10]]\n",
      "Label:  [[-0.00346363 -0.00046981  0.13460311 -0.18464933  0.10797279]]\n",
      "Loss:  Euclidean Loss:  1.27057975624e+23\n",
      "None\n",
      "Test Data:  [-0.5307103  -0.09874414  0.13658755 -0.59129099  0.18496212 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  2.15366395e+09   1.92638592e+09   2.17838756e+10   5.03343219e+11\n",
      "   -1.41664338e+10  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  1.87286856e+10   3.96891341e+09   4.48811008e+10   1.03703314e+12\n",
      "   -2.91869491e+10]]\n",
      "Label:  [[-0.00376296 -0.00050893  0.13418616 -0.19990201  0.11129848]]\n",
      "Loss:  Euclidean Loss:  5.39335238703e+23\n",
      "None\n",
      "Test Data:  [-0.53160183 -0.09894381  0.13629999 -0.59895507  0.19054446 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  4.43717279e+09   3.96891341e+09   4.48811008e+10   1.03703314e+12\n",
      "   -2.91869491e+10  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  3.85865318e+10   8.17711616e+09   9.24681011e+10   2.13658960e+12\n",
      "   -6.01335890e+10]]\n",
      "Label:  [[-0.00406224 -0.0005476   0.13372226 -0.21515469  0.11462416]]\n",
      "Loss:  Euclidean Loss:  2.28936876972e+24\n",
      "None\n",
      "Test Data:  [-0.53256425 -0.09915864  0.13598007 -0.60661915  0.1961268  -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  9.14186468e+09   8.17711616e+09   9.24681011e+10   2.13658960e+12\n",
      "   -6.01335890e+10  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  7.94994688e+10   1.68472402e+10   1.90511153e+11   4.40199545e+12\n",
      "   -1.23892728e+11]]\n",
      "Label:  [[-0.00436127 -0.00058575  0.13320944 -0.23040739  0.11794985]]\n",
      "Loss:  Euclidean Loss:  9.71790618704e+24\n",
      "None\n",
      "Test Data:  [-0.53359751 -0.09938844  0.13562639 -0.61428324  0.20170914 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  1.88348978e+10   1.68472402e+10   1.90511153e+11   4.40199545e+12\n",
      "   -1.23892728e+11  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  1.63791995e+11   3.47101921e+10   3.92508342e+11   9.06938863e+12\n",
      "   -2.55254856e+11]]\n",
      "Label:  [[-0.00466007 -0.00062336  0.13264565 -0.24566007  0.12127554]]\n",
      "Loss:  Euclidean Loss:  4.1250530699e+25\n",
      "None\n",
      "Test Data:  [-0.53470157 -0.09963299  0.13523758 -0.62194732  0.20729148 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  3.88053598e+10   3.47101921e+10   3.92508342e+11   9.06938863e+12\n",
      "   -2.55254856e+11  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  3.37459085e+11   7.15131044e+10   8.08680948e+11   1.86855740e+13\n",
      "   -5.25899137e+11]]\n",
      "Label:  [[-0.00495863 -0.00066034  0.13202892 -0.26091275  0.12460123]]\n",
      "Loss:  Euclidean Loss:  1.75100091863e+26\n",
      "None\n",
      "Test Data:  [-0.53587637 -0.09989205  0.13481225 -0.6296114   0.21287382 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  7.99503128e+10   7.15131044e+10   8.08680948e+11   1.86855740e+13\n",
      "   -5.25899137e+11  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  6.95263691e+11   1.47337757e+11   1.66611806e+12   3.84977129e+13\n",
      "   -1.08350512e+12]]\n",
      "Label:  [[-0.00525707 -0.00069665  0.13135722 -0.27616543  0.12792692]]\n",
      "Loss:  Euclidean Loss:  7.43264402572e+26\n",
      "None\n",
      "Test Data:  [-0.53712185 -0.10016535  0.13434901 -0.63727548  0.21845616 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  1.64720857e+11   1.47337757e+11   1.66611806e+12   3.84977129e+13\n",
      "   -1.08350512e+12  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  1.43244526e+12   3.03558492e+11   3.43268812e+12   7.93164697e+13\n",
      "   -2.23233652e+12]]\n",
      "Label:  [[-0.00555515 -0.00073223  0.13062856 -0.29141811  0.1312526 ]]\n",
      "Loss:  Euclidean Loss:  3.15500655395e+27\n",
      "None\n",
      "Test Data:  [-0.53843798 -0.10045262  0.13384648 -0.64493957  0.2240385  -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  3.39372834e+11   3.03558492e+11   3.43268812e+12   7.93164697e+13\n",
      "   -2.23233652e+12  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  2.95125333e+12   6.25418961e+11   7.07233684e+12   1.63414966e+14\n",
      "   -4.59926300e+12]]\n",
      "Label:  [[-0.00585318 -0.00076702  0.12984091 -0.30667081  0.13457829]]\n",
      "Loss:  Euclidean Loss:  1.33923621705e+28\n",
      "None\n",
      "Test Data:  [-0.53982469 -0.10075353  0.13330328 -0.65260365  0.22962085 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  6.99206621e+11   6.25418961e+11   7.07233684e+12   1.63414966e+14\n",
      "   -4.59926300e+12  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  6.08043899e+12   1.28854445e+12   1.45710708e+13   3.36682285e+14\n",
      "   -9.47581773e+12]]\n",
      "Label:  [[-0.00615072 -0.00080094  0.12899229 -0.32192349  0.13790397]]\n",
      "Loss:  Euclidean Loss:  5.68478477208e+28\n",
      "None\n",
      "Test Data:  [-0.54128193 -0.10106775  0.13271802 -0.66026773  0.23520319 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  1.44056871e+12   1.28854445e+12   1.45710708e+13   3.36682285e+14\n",
      "   -9.47581773e+12  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  1.25274696e+13   2.65477685e+12   3.00206051e+13   6.93663257e+14\n",
      "   -1.95229416e+13]]\n",
      "Label:  [[-0.00644827 -0.00083394  0.1280807  -0.33717617  0.14122966]]\n",
      "Loss:  Euclidean Loss:  2.41307543777e+29\n",
      "None\n",
      "Test Data:  [-0.54280965 -0.10139492  0.13208934 -0.66793181  0.24078553 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  2.96798977e+12   2.65477685e+12   3.00206051e+13   6.93663257e+14\n",
      "   -1.95229416e+13  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  2.58102243e+13   5.46960677e+12   6.18511781e+13   1.42914768e+15\n",
      "   -4.02228888e+13]]\n",
      "Label:  [[-0.00674558 -0.00086595  0.12710412 -0.35242885  0.14455535]]\n",
      "Loss:  Euclidean Loss:  1.02430131297e+30\n",
      "None\n",
      "Test Data:  [-0.54440778 -0.10173464  0.13141584 -0.6755959   0.24636787 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  6.11492058e+12   5.46960677e+12   6.18511781e+13   1.42914768e+15\n",
      "   -4.02228888e+13  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  5.31765521e+13   1.12689792e+13   1.27431311e+14   2.94445939e+15\n",
      "   -8.28707900e+13]]\n",
      "Label:  [[-0.00704253 -0.00089689  0.12606058 -0.36768153  0.14788103]]\n",
      "Loss:  Euclidean Loss:  4.34795097397e+30\n",
      "None\n",
      "Test Data:  [-0.54607629 -0.10208651  0.13069615 -0.68325998  0.25195021 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  1.25985109e+13   1.12689792e+13   1.27431311e+14   2.94445939e+15\n",
      "   -8.28707900e+13  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  1.09559146e+14   2.32173664e+13   2.62545579e+14   6.06644159e+15\n",
      "   -1.70737902e+14]]\n",
      "Label:  [[-0.00733924 -0.00092668  0.12494805 -0.38293421  0.15120673]]\n",
      "Loss:  Euclidean Loss:  1.84561687838e+31\n",
      "None\n",
      "Test Data:  [-0.5478151  -0.10245006  0.1299289  -0.69092406  0.25753255 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  2.59565927e+13   2.32173664e+13   2.62545579e+14   6.06644159e+15\n",
      "   -1.70737902e+14  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  2.25723667e+14   4.78345188e+13   5.40920261e+14   1.24986308e+16\n",
      "   -3.51769901e+14]]\n",
      "Label:  [[-0.00763583 -0.00095526  0.12376457 -0.39818689  0.15453242]]\n",
      "Loss:  Euclidean Loss:  7.83426727084e+31\n",
      "None\n",
      "Test Data:  [-0.54962416 -0.10282482  0.12911271 -0.69858814  0.26311489 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  5.34781210e+13   4.78345188e+13   5.40920261e+14   1.24986308e+16\n",
      "   -3.51769901e+14  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  4.65056307e+14   9.85530578e+13   1.11445355e+15   2.57508114e+16\n",
      "   -7.24748351e+14]]\n",
      "Label:  [[-0.00793207 -0.00098255  0.12250813 -0.4134396   0.1578581 ]]\n",
      "Loss:  Euclidean Loss:  3.32548761208e+32\n",
      "None\n",
      "Test Data:  [-0.55150341 -0.10321029  0.1282462  -0.70625223  0.26869723 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  1.10180460e+14   9.85530578e+13   1.11445355e+15   2.57508114e+16\n",
      "   -7.24748351e+14  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  9.58151101e+14   2.03047817e+14   2.29609757e+15   5.30541505e+16\n",
      "   -1.49319209e+15]]\n",
      "Label:  [[-0.00822806 -0.00100847  0.12117674 -0.42869228  0.16118379]]\n",
      "Loss:  Euclidean Loss:  1.41160198182e+33\n",
      "None\n",
      "Test Data:  [-0.5534528  -0.10360593  0.12732801 -0.71391631  0.27427957 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  2.27003757e+14   2.03047817e+14   2.29609757e+15   5.30541505e+16\n",
      "   -1.49319209e+15  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  1.97406978e+15   4.18337465e+14   4.73062945e+15   1.09306952e+17\n",
      "   -3.07640776e+15]]\n",
      "Label:  [[-0.00852382 -0.00103292  0.11976843 -0.44394496  0.16450948]]\n",
      "Loss:  Euclidean Loss:  5.99196217704e+33\n",
      "None\n",
      "Test Data:  [-0.55547226 -0.10401116  0.12635676 -0.72158039  0.27986191 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[  4.67693724e+14   4.18337465e+14   4.73062945e+15   1.09306952e+17\n",
      "   -3.07640776e+15  -7.62685563e-01   8.31416416e-01   2.00000000e-02]]\n",
      "Pred:  [[  4.06715633e+15   8.61897394e+14   9.74647265e+15   2.25204068e+17\n",
      "   -6.33829369e+15]]\n",
      "Label:  [[-0.00881946 -0.00105584  0.11828118 -0.45919764  0.16783516]]\n",
      "Loss:  Euclidean Loss:  2.54346615778e+34\n",
      "None\n",
      "Test Data:  [-0.5264437  -0.09778059  0.13787201 -0.46100159  0.09006232 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  9.63584727e+14   8.61897394e+14   9.74647265e+15   2.25204068e+17\n",
      "   -6.33829369e+15  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  8.37952361e+15   1.77575812e+15   2.00805655e+16   4.63985764e+17\n",
      "   -1.30587321e+16]]\n",
      "Label:  [[ 0.00132149  0.00018151  0.13653553  0.05578303  0.05376918]]\n",
      "Loss:  Euclidean Loss:  1.07964959822e+35\n",
      "None\n",
      "Test Data:  [-0.52613061 -0.09770939  0.13792024 -0.47047989  0.09397852 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  1.98526447e+15   1.77575812e+15   2.00805655e+16   4.63985764e+17\n",
      "   -1.30587321e+16  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  1.72642513e+16   3.65857770e+15   4.13717922e+16   9.55945371e+17\n",
      "   -2.69047575e+16]]\n",
      "Label:  [[  9.48131084e-04   1.30280852e-04   1.36587948e-01   3.69197801e-02\n",
      "    5.61022609e-02]]\n",
      "Loss:  Euclidean Loss:  4.58289245109e+35\n",
      "None\n",
      "Test Data:  [-0.52590598 -0.09765827  0.13795639 -0.47995819  0.09789471 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  4.09022117e+15   3.65857770e+15   4.13717922e+16   9.55945371e+17\n",
      "   -2.69047575e+16  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  3.55693730e+16   7.53773418e+15   8.52378522e+16   1.96952522e+18\n",
      "   -5.54316554e+16]]\n",
      "Label:  [[  5.74827194e-04   7.90208578e-05   1.36621058e-01   1.80565231e-02\n",
      "    5.84353358e-02]]\n",
      "Loss:  Euclidean Loss:  1.94534485084e+36\n",
      "None\n",
      "Test Data:  [-0.52576979 -0.09762727  0.13797923 -0.48943649  0.1018109  -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  8.42704381e+15   7.53773418e+15   8.52378522e+16   1.96952522e+18\n",
      "   -5.54316554e+16  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  7.32832403e+16   1.55299070e+16   1.75614684e+17   4.05779384e+18\n",
      "   -1.14205208e+17]]\n",
      "Label:  [[  2.01702118e-04   2.77236104e-05   1.36633143e-01  -8.06732511e-04\n",
      "    6.07684143e-02]]\n",
      "Loss:  Euclidean Loss:  8.25759326757e+36\n",
      "None\n",
      "Test Data:  [-0.52572201 -0.0976164   0.13798756 -0.4989148   0.10572709 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  1.73621581e+16   1.55299070e+16   1.75614684e+17   4.05779384e+18\n",
      "   -1.14205208e+17  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  1.50984765e+17   3.19961449e+16   3.61817391e+17   8.36023452e+18\n",
      "   -2.35296176e+17]]\n",
      "Label:  [[ -1.71363354e-04  -2.35587358e-05   1.36622414e-01  -1.96699891e-02\n",
      "    6.31014928e-02]]\n",
      "Loss:  Euclidean Loss:  3.50518118175e+37\n",
      "None\n",
      "Test Data:  [-0.52576261 -0.09762564  0.13798016 -0.5083931   0.10964329 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  3.57710896e+16   3.19961449e+16   3.61817391e+17   8.36023452e+18\n",
      "   -2.35296176e+17  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  3.11072421e+17   6.59213399e+16   7.45448818e+17   1.72245116e+19\n",
      "   -4.84778525e+17]]\n",
      "Label:  [[ -5.44369221e-04  -7.48261809e-05   1.36587128e-01  -3.85332443e-02\n",
      "    6.54345676e-02]]\n",
      "Loss:  Euclidean Loss:  1.48787811924e+38\n",
      "None\n",
      "Test Data:  [-0.52589158 -0.097655    0.13795583 -0.5178714   0.11355948 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  7.36988229e+16   6.59213399e+16   7.45448818e+17   1.72245116e+19\n",
      "   -4.84778525e+17  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  6.40899693e+17   1.35817174e+17   1.53584127e+18   3.54874964e+19\n",
      "   -9.98785368e+17]]\n",
      "Label:  [[ -9.17196274e-04  -1.26041472e-04   1.36525527e-01  -5.73965013e-02\n",
      "    6.77676424e-02]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.52610888 -0.09770444  0.13791334 -0.5273497   0.11747567 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  1.51841018e+17   1.35817174e+17   1.53584127e+18   3.54874964e+19\n",
      "   -9.98785368e+17  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  1.32043952e+18   2.79822531e+17   3.16427902e+18   7.31145458e+19\n",
      "   -2.05778659e+18]]\n",
      "Label:  [[-0.0012899  -0.00017716  0.13643585 -0.07625975  0.07010072]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.52641449 -0.09777395  0.1378515  -0.53682801  0.12139186 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  3.12836600e+17   2.79822531e+17   3.16427902e+18   7.31145458e+19\n",
      "   -2.05778659e+18  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  2.72048864e+18   5.76516003e+17   6.51933100e+18   1.50637192e+20\n",
      "   -4.23963437e+18]]\n",
      "Label:  [[-0.00166255 -0.00022817  0.13631636 -0.09512302  0.0724338 ]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.52680838 -0.09786346  0.13776909 -0.54630631  0.12530806 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  6.44534188e+17   5.76516003e+17   6.51933100e+18   1.50637192e+20\n",
      "   -4.23963437e+18  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  5.60499692e+18   1.18779032e+18   1.34317132e+19   3.10356350e+20\n",
      "   -8.73488321e+18]]\n",
      "Label:  [[-0.00203508 -0.00027902  0.13616528 -0.11398627  0.07476687]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.52729052 -0.09797292  0.13766489 -0.55578461  0.12922425 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  1.32792767e+18   1.18779032e+18   1.34317132e+19   3.10356350e+20\n",
      "   -8.73488321e+18  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  1.15479188e+19   2.44719183e+18   2.76732343e+19   6.39424171e+20\n",
      "   -1.79963841e+19]]\n",
      "Label:  [[-0.00240749 -0.00032967  0.13598084 -0.13284953  0.07709996]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.52786089 -0.09810225  0.1375377  -0.56526291  0.13314044 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  2.73591604e+18   2.44719183e+18   2.76732343e+19   6.39424171e+20\n",
      "   -1.79963841e+19  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  2.37920682e+19   5.04192932e+18   5.70149028e+19   1.31739944e+21\n",
      "   -3.70778103e+19]]\n",
      "Label:  [[-0.00277972 -0.00038008  0.13576134 -0.15171278  0.07943303]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.52851947 -0.09825137  0.13738631 -0.57474122  0.13705663 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  5.63678200e+18   5.04192932e+18   5.70149028e+19   1.31739944e+21\n",
      "   -3.70778103e+19  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  4.90185637e+19   1.03878318e+19   1.17467231e+20   2.71422548e+21\n",
      "   -7.63909717e+19]]\n",
      "Label:  [[-0.00315189 -0.00043022  0.13550496 -0.17057604  0.08176611]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.52926621 -0.09842015  0.13720951 -0.58421952  0.14097283 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  1.16134064e+19   1.03878318e+19   1.17467231e+20   2.71422548e+21\n",
      "   -7.63909717e+19  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  1.00992465e+20   2.14019499e+19   2.42016618e+20   5.59209264e+21\n",
      "   -1.57387648e+20]]\n",
      "Label:  [[-0.00352401 -0.00048002  0.13521001 -0.1894393   0.08409919]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.53010111 -0.09860847  0.13700609 -0.59369782  0.14488902 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  2.39269871e+19   2.14019499e+19   2.42016618e+20   5.59209264e+21\n",
      "   -1.57387648e+20  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  2.08073832e+20   4.40942426e+19   4.98625040e+20   1.15213360e+22\n",
      "   -3.24264240e+20]]\n",
      "Label:  [[-0.00389588 -0.00052945  0.13487469 -0.20830254  0.08643226]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.53102412 -0.09881618  0.13677484 -0.60317612  0.14880521 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  4.92965479e+19   4.40942426e+19   4.98625040e+20   1.15213360e+22\n",
      "   -3.24264240e+20  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  4.28692547e+20   9.08469635e+19   1.02731245e+21   2.37372982e+22\n",
      "   -6.68078887e+20]]\n",
      "Label:  [[-0.00426769 -0.00057844  0.13449727 -0.2271658   0.08876534]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.53203522 -0.09904311  0.13651455 -0.61265443  0.1527214  -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  1.01565211e+20   9.08469635e+19   1.02731245e+21   2.37372982e+22\n",
      "   -6.68078887e+20  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  8.83231111e+20   1.87171219e+20   2.11656122e+21   4.89057278e+22\n",
      "   -1.37643572e+21]]\n",
      "Label:  [[-0.00463945 -0.00062696  0.13407598 -0.24602906  0.09109842]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.53313438 -0.09928907  0.13622401 -0.62213273  0.1566376  -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  2.09253823e+20   1.87171219e+20   2.11656122e+21   4.89057278e+22\n",
      "   -1.37643572e+21  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  1.81971264e+21   3.85626981e+20   4.36073194e+21   1.00760007e+23\n",
      "   -2.83585701e+21]]\n",
      "Label:  [[-0.00501102 -0.00067493  0.1336091  -0.26489231  0.0934315 ]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.53432158 -0.09955385  0.13590202 -0.63161103  0.16055379 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  4.31123657e+20   3.85626981e+20   4.36073194e+21   1.00760007e+23\n",
      "   -2.83585701e+21  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  3.74913635e+21   7.94503232e+20   8.98437276e+21   2.07594866e+23\n",
      "   -5.84268868e+21]]\n",
      "Label:  [[-0.00538248 -0.00072229  0.13309486 -0.28375557  0.09576457]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.5355968  -0.09983722  0.13554738 -0.64108933  0.16446998 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  8.88239900e+20   7.94503232e+20   8.98437276e+21   2.07594866e+23\n",
      "   -5.84268868e+21  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  7.72431006e+21   1.63690703e+21   1.85104182e+22   4.27705704e+23\n",
      "   -1.20376264e+22]]\n",
      "Label:  [[-0.00575387 -0.000769    0.13253152 -0.30261883  0.09809764]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.53696    -0.10013891  0.13515886 -0.65056764  0.16838617 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  1.83003224e+21   1.63690703e+21   1.85104182e+22   4.27705704e+23\n",
      "   -1.20376264e+22  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  1.59143238e+22   3.37250538e+21   3.81368374e+22   8.81197938e+23\n",
      "   -2.48010259e+22]]\n",
      "Label:  [[-0.00612515 -0.00081497  0.13191731 -0.32148209  0.10043073]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.53841116 -0.10045863  0.13473527 -0.66004594  0.17230237 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  3.77039832e+21   3.37250538e+21   3.81368374e+22   8.81197938e+23\n",
      "   -2.48010259e+22  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  3.27881261e+22   6.94834153e+21   7.85729516e+22   1.81552365e+24\n",
      "   -5.10972649e+22]]\n",
      "Label:  [[-0.00649625 -0.00086015  0.1312505  -0.34034535  0.1027638 ]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.53995025 -0.10079608  0.13427541 -0.66952424  0.17621856 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  7.76811486e+21   6.94834153e+21   7.85729516e+22   1.81552365e+24\n",
      "   -5.10972649e+22  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  6.75530577e+22   1.43155989e+22   1.61883042e+23   3.74050625e+24\n",
      "   -1.05275388e+23]]\n",
      "Label:  [[-0.00686741 -0.00090446  0.13052934 -0.35920858  0.10509688]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.54157725 -0.10115092  0.13377805 -0.67900254  0.18013475 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  1.60045716e+22   1.43155989e+22   1.61883042e+23   3.74050625e+24\n",
      "   -1.05275388e+23  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  1.39178928e+23   2.94942811e+22   3.33526356e+23   7.70652855e+24\n",
      "   -2.16897682e+23]]\n",
      "Label:  [[-0.00723827 -0.00094784  0.12975208 -0.37807184  0.10742996]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.54329214 -0.10152277  0.13324201 -0.68848085  0.18405094 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  3.29740679e+22   2.94942811e+22   3.33526356e+23   7.70652855e+24\n",
      "   -2.16897682e+23  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  2.86748998e+23   6.07667951e+22   6.87161321e+23   1.58776868e+25\n",
      "   -4.46872087e+23]]\n",
      "Label:  [[-0.00760913 -0.0009902   0.12891696 -0.39693511  0.10976303]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.54509489 -0.10191124  0.13266608 -0.69795915  0.18796714 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  6.79361532e+22   6.07667951e+22   6.87161321e+23   1.58776868e+25\n",
      "   -4.46872087e+23  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  5.90786198e+23   1.25197304e+23   1.41575187e+24   3.27126489e+25\n",
      "   -9.20686220e+23]]\n",
      "Label:  [[-0.00797999 -0.00103147  0.12802227 -0.41579837  0.11209611]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.54698548 -0.1023159   0.13204905 -0.70743745  0.19188333 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  1.39968202e+23   1.25197304e+23   1.41575187e+24   3.27126489e+25\n",
      "   -9.20686220e+23  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  1.21719097e+24   2.57942804e+23   2.91685884e+24   6.73975591e+25\n",
      "   -1.89688100e+24]]\n",
      "Label:  [[-0.00835061 -0.00107157  0.12706624 -0.43466163  0.11442919]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.54896389 -0.1027363   0.13138971 -0.71691575  0.19579952 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  2.88375104e+23   2.57942804e+23   2.91685884e+24   6.73975591e+25\n",
      "   -1.89688100e+24  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  2.50776712e+24   5.31437150e+23   6.00957337e+24   1.38858530e+26\n",
      "   -3.90812720e+24]]\n",
      "Label:  [[-0.00872111 -0.00111042  0.12604712 -0.45352489  0.11676227]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.55103009 -0.10317193  0.13068688 -0.72639406  0.19971571 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  5.94136518e+23   5.31437150e+23   6.00957337e+24   1.38858530e+26\n",
      "   -3.90812720e+24  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  5.16672895e+24   1.09491456e+24   1.23814662e+25   2.86088931e+26\n",
      "   -8.05188158e+24]]\n",
      "Label:  [[-0.00909162 -0.00114793  0.12496318 -0.47238812  0.11909534]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.55318407 -0.10362228  0.12993934 -0.73587236  0.20363191 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  1.22409386e+24   1.09491456e+24   1.23814662e+25   2.86088931e+26\n",
      "   -8.05188158e+24  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  1.06449623e+25   2.25584224e+24   2.55094121e+25   5.89426305e+26\n",
      "   -1.65892077e+25]]\n",
      "Label:  [[-0.009462   -0.001184    0.12381268 -0.49125138  0.12142842]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.55542579 -0.10408678  0.12914589 -0.74535066  0.2075481  -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  2.52198890e+24   2.25584224e+24   2.55094121e+25   5.89426305e+26\n",
      "   -1.65892077e+25  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  2.19317168e+25   4.64768859e+24   5.25567661e+25   1.21438938e+27\n",
      "   -3.41786070e+25]]\n",
      "Label:  [[-0.00983226 -0.00121857  0.12259386 -0.51011467  0.1237615 ]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.55775525 -0.10456484  0.12830533 -0.75482896  0.21146429 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  5.19603026e+24   4.64768859e+24   5.25567661e+25   1.21438938e+27\n",
      "   -3.41786070e+25  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  4.51857054e+25   9.57558840e+24   1.08282203e+26   2.50199477e+27\n",
      "   -7.04178445e+25]]\n",
      "Label:  [[-0.01020253 -0.00125153  0.121305   -0.52897787  0.12609458]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.56017242 -0.10505583  0.12741647 -0.76430727  0.21538048 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:371: RuntimeWarning: overflow encountered in square\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.07053312e+25   9.57558840e+24   1.08282203e+26   2.50199477e+27\n",
      "   -7.04178445e+25  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  9.30956722e+25   1.97284884e+25   2.23092894e+26   5.15483549e+27\n",
      "   -1.45081133e+26]]\n",
      "Label:  [[-0.01057267 -0.00128278  0.11994436 -0.54784113  0.12842765]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.56267727 -0.10555909  0.12647809 -0.77378557  0.21929668 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[  2.20560905e+25   1.97284884e+25   2.23092894e+26   5.15483549e+27\n",
      "   -1.45081133e+26  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[  1.91804098e+26   4.06464321e+25   4.59636415e+26   1.06204582e+28\n",
      "   -2.98909344e+26]]\n",
      "Label:  [[-0.0109427  -0.00131223  0.11851019 -0.56670439  0.13076073]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.5264437  -0.09778059  0.13787201 -0.46100159  0.09006232 -0.86892124\n",
      "  0.64540666  0.02      ]\n",
      "Input:  [[  4.54419464e+25   4.06464321e+25   4.59636415e+26   1.06204582e+28\n",
      "   -2.98909344e+26  -8.68921238e-01   6.45406665e-01   2.00000000e-02]]\n",
      "Pred:  [[  3.95172202e+26   8.37434681e+25   9.46984563e+26   2.18812291e+28\n",
      "   -6.15840124e+26]]\n",
      "Label:  [[ 0.00133371  0.00018319  0.13653632  0.05726898  0.05401786]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.52612771 -0.09770873  0.13792078 -0.46973324  0.09439593 -0.86892124\n",
      "  0.64540666  0.02      ]\n",
      "Input:  [[  9.36236200e+25   8.37434681e+25   9.46984563e+26   2.18812291e+28\n",
      "   -6.15840124e+26  -8.68921238e-01   6.45406665e-01   2.00000000e-02]]\n",
      "Pred:  [[  8.14169407e+26   1.72535976e+26   1.95106313e+27   4.50816891e+28\n",
      "   -1.26880838e+27]]\n",
      "Label:  [[  9.89794731e-04   1.36010349e-04   1.36591390e-01   3.98916714e-02\n",
      "    5.65996058e-02]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.52589322 -0.09765537  0.13795876 -0.47846489  0.09872953 -0.86892124\n",
      "  0.64540666  0.02      ]\n",
      "Input:  [[  1.92891825e+26   1.72535976e+26   1.95106313e+27   4.50816891e+28\n",
      "   -1.26880838e+27  -8.68921238e-01   6.45406665e-01   2.00000000e-02]]\n",
      "Pred:  [[  1.67742575e+27   3.55473960e+26   4.01975745e+27   9.28813740e+28\n",
      "   -2.61411437e+27]]\n",
      "Label:  [[  6.45875931e-04   8.87811184e-05   1.36629000e-01   2.25143619e-02\n",
      "    5.91813549e-02]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.5257402  -0.09762054  0.1379847  -0.48719654  0.10306313 -0.86892124\n",
      "  0.64540666  0.02      ]\n",
      "Input:  [[  3.97413254e+26   3.55473960e+26   4.01975745e+27   9.28813740e+28\n",
      "   -2.61411437e+27  -8.68921238e-01   6.45406665e-01   2.00000000e-02]]\n",
      "Pred:  [[  3.45598422e+27   7.32380012e+26   8.28187029e+27   1.91362606e+29\n",
      "   -5.38583772e+27]]\n",
      "Label:  [[  3.02076340e-04   4.15369868e-05   1.36647359e-01   5.13705146e-03\n",
      "    6.17631078e-02]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.52566864 -0.09760424  0.13799736 -0.4959282   0.10739674 -0.86892124\n",
      "  0.64540666  0.02      ]\n",
      "Input:  [[  8.18786726e+26   7.32380012e+26   8.28187029e+27   1.91362606e+29\n",
      "   -5.38583772e+27  -8.68921238e-01   6.45406665e-01   2.00000000e-02]]\n",
      "Pred:  [[  7.12033165e+27   1.50891592e+27   1.70630529e+28   3.94262520e+29\n",
      "   -1.10963901e+28]]\n",
      "Label:  [[ -4.16040421e-05  -5.72204590e-06   1.36644691e-01  -1.22402580e-02\n",
      "    6.43448532e-02]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.52567849 -0.09760649  0.13799552 -0.50465985  0.11173034 -0.86892124\n",
      "  0.64540666  0.02      ]\n",
      "Input:  [[  1.68693856e+27   1.50891592e+27   1.70630529e+28   3.94262520e+29\n",
      "   -1.10963901e+28  -8.68921238e-01   6.45406665e-01   2.00000000e-02]]\n",
      "Pred:  [[  1.46699488e+28   3.10880174e+27   3.51548592e+28   8.12295241e+29\n",
      "   -2.28617884e+28]]\n",
      "Label:  [[ -3.85165215e-04  -5.29512763e-05   1.36619180e-01  -2.96175685e-02\n",
      "    6.69266060e-02]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.52576974 -0.09762726  0.13797793 -0.5133915   0.11606394 -0.86892124\n",
      "  0.64540666  0.02      ]\n",
      "Input:  [[  3.47558282e+27   3.10880174e+27   3.51548592e+28   8.12295241e+29\n",
      "   -2.28617884e+28  -8.68921238e-01   6.45406665e-01   2.00000000e-02]]\n",
      "Pred:  [[  3.02243544e+28   6.40504011e+27   7.24292393e+28   1.67356407e+30\n",
      "   -4.71019033e+28]]\n",
      "Label:  [[ -7.28607178e-04  -1.00150704e-04   1.36569068e-01  -4.69948761e-02\n",
      "    6.95083588e-02]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n",
      "Test Data:  [-0.52594236 -0.09766655  0.13794337 -0.52212315  0.12039754 -0.86892124\n",
      "  0.64540666  0.02      ]\n",
      "Input:  [[  7.16070983e+27   6.40504011e+27   7.24292393e+28   1.67356407e+30\n",
      "   -4.71019033e+28  -8.68921238e-01   6.45406665e-01   2.00000000e-02]]\n",
      "Pred:  [[  6.22709378e+28   1.31962352e+28   1.49225194e+29   3.44802780e+30\n",
      "   -9.70435734e+28]]\n",
      "Label:  [[-0.00107193 -0.00014728  0.13649254 -0.06437219  0.0720901 ]]\n",
      "Loss:  Euclidean Loss:  inf\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# main()\n",
    "main(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.asarray([1, 1, 1, 1, 1])\n",
    "b = np.asarray([2, 2, 2, 2, 2, 2, 2])\n",
    "b[0]\n",
    "np.asarray([np.concatenate((a, b[5:8]), axis=0)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29162415862083435, 0.008049188181757927, 0.033666692674160004, 0.27424606680870056, 0.008417674340307713, 0.05690521001815796, 0.0005845790728926659, 0.0]\n",
      "[0.6776861548423767, -0.06710384786128998, -0.05981121584773064, 0.4085806608200073, -0.004740194883197546, 0.04174768552184105, 0.0003070685488637537, 0.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "blob = caffe.proto.caffe_pb2.BlobProto()\n",
    "data = open( \"toycar_mean.binaryproto\" , 'rb' ).read()\n",
    "blob.ParseFromString(data)\n",
    "# arr = np.array( caffe.io.blobproto_to_array(blob) )\n",
    "# out = arr[0]\n",
    "print blob.data\n",
    "\n",
    "data = open( \"../prx_ws/src/prx_learn/data/toy_car/toy_car_mean.binaryproto\" , 'rb' ).read()\n",
    "blob.ParseFromString(data)\n",
    "print blob.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
