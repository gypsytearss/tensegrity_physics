{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Requirements:\n",
    " - Caffe (script to install Caffe and pycaffe on a new Ubuntu 14.04 LTS x64 or Ubuntu 14.10 x64. \n",
    "   CPU only, multi-threaded Caffe. http://stackoverflow.com/a/31396229/395857)\n",
    " - sudo pip install pydot\n",
    " - sudo apt-get install -y graphviz\n",
    "\n",
    "Interesting resources on Caffe:\n",
    " - https://github.com/BVLC/caffe/tree/master/examples\n",
    " - http://nbviewer.ipython.org/github/joyofdata/joyofdata-articles\\\n",
    " /blob/master/deeplearning-with-caffe/\\\n",
    " Neural-Networks-with-Caffe-on-the-GPU.ipynb\n",
    "'''\n",
    "\n",
    "import subprocess\n",
    "import platform\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# from sklearn.datasets import load_iris\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import caffe\n",
    "import caffe.draw\n",
    "import google.protobuf \n",
    "\n",
    "# Globals\n",
    "train_data, train_labels, test_data, test_labels = [], [], [], []\n",
    "\n",
    "def load_data():\n",
    "    '''\n",
    "    Load Sample for Forward Pass from Toy Car Data set\n",
    "    '''\n",
    "    start_states, controls, durations, end_states = [], [], [], []\n",
    "\n",
    "    with open('data_output_50Hz.txt', 'r') as infile:\n",
    "        data = infile.readlines()\n",
    "\n",
    "        idx, i = 0, 0\n",
    "\n",
    "        for line in data:\n",
    "            # Stop at end of file\n",
    "            if line == '':\n",
    "                break\n",
    "\n",
    "            # Reset and continue at Trajectory break\n",
    "            if len(line) == 1:\n",
    "                start_states.pop()\n",
    "                i = 0\n",
    "                if idx > 1000000:\n",
    "                    break\n",
    "                continue\n",
    "                \n",
    "            # Split Values in line and append to individual lists\n",
    "            vals = line.split(',')\n",
    "            if i % 3 == 0:\n",
    "                start_states.append([float(val) for val in vals])\n",
    "                if i != 0:\n",
    "                    end_states.append([float(val) for val in vals])\n",
    "                    idx += 1\n",
    "            elif i % 3 == 1:\n",
    "                controls.append([float(val) for val in vals])\n",
    "            elif i % 3 == 2:\n",
    "                durations.append([float(val) for val in vals])\n",
    "                \n",
    "            i += 1\n",
    "            \n",
    "    X = np.concatenate((start_states, controls, durations), axis=1)\n",
    "    start_states = np.asarray(start_states, dtype=np.float32)\n",
    "    end_states = np.asarray(end_states, dtype=np.float32)\n",
    "    \n",
    "    X, meanx, minx, maxx = normalize_data(X)\n",
    "    y = normalize_labels(start_states, end_states)\n",
    "\n",
    "    # Shuffle the data around and split 3M training, ~750k validation\n",
    "    indices = np.random.permutation(X.shape[0])\n",
    "    training_idx = indices[:900000]\n",
    "    \n",
    "    train_X = X[training_idx, :]\n",
    "    train_y = y[training_idx, :]\n",
    "\n",
    "    test_X = X[900000:1000000, :]\n",
    "    test_y = y[900000:1000000, :]\n",
    "\n",
    "    return train_X, train_y, test_X, test_y, meanx, minx, maxx\n",
    "\n",
    "\n",
    "def write_binaryproto(data, string):\n",
    "    blob = caffe.proto.caffe_pb2.BlobProto()\n",
    "    blob.channels = data.shape[0]\n",
    "    blob.data.extend(data.astype(float).flat)\n",
    "    binaryproto_file = open('toycar_' + string + '.binaryproto', 'wb')\n",
    "    binaryproto_file.write(blob.SerializeToString())\n",
    "    binaryproto_file.close()\n",
    "\n",
    "\n",
    "def save_binaryproto(data, ftype='mean'):\n",
    "    '''\n",
    "    Take the mean values of the raw data and store them as binaryproto type\n",
    "    In order to use them later for deploy normalization\n",
    "    '''\n",
    "    # Convert to 32bit float\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "\n",
    "    # Set project home dir \n",
    "    PROJECT_HOME = os.path.abspath('.')\n",
    "\n",
    "    # Initialize blob to store serialized means\n",
    "    blob = caffe.proto.caffe_pb2.BlobProto()\n",
    "\n",
    "    # Custom dimensions for blob for this project\n",
    "    blob.num = 1\n",
    "    blob.channels = data.shape[0]\n",
    "    blob.height = 1\n",
    "    blob.width = 1\n",
    "    \n",
    "    # Reshape data and copy into blob\\n\",\n",
    "    blob.data.extend(data.astype(float).flat)\n",
    "    \n",
    "    # Write file\n",
    "    binaryproto_file = open(PROJECT_HOME + '/toycar_' + ftype + '.binaryproto', 'wb')\n",
    "    binaryproto_file.write(blob.SerializeToString())\n",
    "    binaryproto_file.close()\n",
    "\n",
    "\n",
    "\n",
    "def normalize_labels(start_states, end_states):\n",
    "    '''\n",
    "    Normalize end states such that positional coordinates e.g. (x,y)\n",
    "    are now represented by delta_(x,y)\n",
    "    '''\n",
    "    y = end_states\n",
    "    y[:, 0] = end_states[:, 0] - start_states[:, 0]\n",
    "    y[:, 1] = end_states[:, 1] - start_states[:, 1]\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def unnormalize_data(data, meanx, minx, maxx):\n",
    "    desired_min = -1\n",
    "    desired_max = 1\n",
    "    desired_rng = desired_max - desired_min\n",
    "    \n",
    "    data = data - desired_min\n",
    "    for i in range(0, data.shape[1]):\n",
    "        data[:,i] = data[:,i] * (maxx[:,i] - minx[:,i]) \\\n",
    "                    / desired_rng + minx[:,i] + meanx[:,i]\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def normalize_data(data):\n",
    "    '''\n",
    "    Normalize data to zero mean on [-1,1] interval for all dimensions\n",
    "    '''\n",
    "    X_mean = np.mean(data, axis=0)\n",
    "    \n",
    "    # do not substract duration\n",
    "    X_mean[7] = 0\n",
    "    \n",
    "    # Mean Shift\n",
    "    data_t = data - X_mean\n",
    "    \n",
    "    # Find bounds, define desired bounds\n",
    "    X_min = np.min(data_t, axis=0)\n",
    "    X_max = np.max(data_t, axis=0)\n",
    "\n",
    "    # Write 'em Out\n",
    "    save_binaryproto(X_mean, ftype=\"mean\")\n",
    "    save_binaryproto(X_min, ftype=\"min\")\n",
    "    save_binaryproto(X_max, ftype=\"max\")\n",
    "\n",
    "    desiredMin = -1\n",
    "    desiredMax = 1\n",
    "    \n",
    "    # Normalize \n",
    "    for i in range(0, 7):\n",
    "        data_t[:, i] = (data_t[:, i] - X_min[i]) * (desiredMax - desiredMin)\\\n",
    "            / (X_max[i] - X_min[i]) + desiredMin\n",
    "\n",
    "    return data_t, X_mean, X_min, X_max\n",
    "\n",
    "\n",
    "# def normalize_data(data, X_mean, X_min, X_max):\n",
    "#     '''\n",
    "#     Normalize data to zero mean on [-1,1] interval for all dimensions\n",
    "#     '''\n",
    "# #     X_mean = np.mean(data, axis=0)\n",
    "    \n",
    "#     # do not substract duration\n",
    "# #     X_mean[7] = 0\n",
    "    \n",
    "#     # Mean Shift\n",
    "#     data_t = data - X_mean\n",
    "    \n",
    "#     # Find bounds, define desired bounds\n",
    "#     X_min = np.min(data_t, axis=0)\n",
    "#     X_max = np.max(data_t, axis=0)\n",
    "\n",
    "#     # Write 'em Out\n",
    "# #     save_binaryproto(X_mean, ftype=\"mean\")\n",
    "# #     save_binaryproto(X_min, ftype=\"min\")\n",
    "# #     save_binaryproto(X_max, ftype=\"max\")\n",
    "\n",
    "#     desiredMin = -1\n",
    "#     desiredMax = 1\n",
    "    \n",
    "#     # Normalize \n",
    "#     for i in range(0, 7):\n",
    "#         data_t[:, i] = (data_t[:, i] - X_min[i]) * (desiredMax - desiredMin)\\\n",
    "#             / (X_max[i] - X_min[i]) + desiredMin\n",
    "\n",
    "#     return data_t, X_mean, X_min, X_max\n",
    "\n",
    "def normalize_test_data(data, meanx, minx, maxx):\n",
    "    '''\n",
    "    Normalize data to zero mean on [-1,1] interval for all dimensions\n",
    "    '''\n",
    "    \n",
    "    # Mean Shift\n",
    "#     print \"data: \", data.shape\n",
    "#     print \"mean: \", meanx.shape\n",
    "#     print \"min: \", minx.shape\n",
    "#     print \"max: \", maxx.shape\n",
    "    \n",
    "#     print \"DATA\", data\n",
    "#     print \"MEAN\", meanx\n",
    "    \n",
    "    data_t = data - meanx\n",
    "    \n",
    "#     print \"DATA - MEAN: \", data_t\n",
    "    \n",
    "    desiredMin = -1\n",
    "    desiredMax = 1\n",
    "    \n",
    "    # Normalize \n",
    "    for i in range(0, data.shape[1]):\n",
    "        data_t[:, i] = (data_t[:, i] - minx[:, i]) * (desiredMax - desiredMin)\\\n",
    "            / (maxx[:, i] - minx[:, i]) + desiredMin\n",
    "\n",
    "    return data_t\n",
    "\n",
    "\n",
    "def save_data_as_hdf5(hdf5_data_filename, data, labels):\n",
    "    '''\n",
    "    HDF5 is one of the data formats Caffe accepts\n",
    "    '''\n",
    "    with h5py.File(hdf5_data_filename, 'w') as f:\n",
    "        f['data'] = data.astype(np.float32)\n",
    "        f['label'] = labels.astype(np.float32)\n",
    "\n",
    "\n",
    "def train(solver_prototxt_filename):\n",
    "    '''\n",
    "    Train the ANN\n",
    "    '''\n",
    "    # caffe.set_mode_gpu()\n",
    "    solver = caffe.get_solver(solver_prototxt_filename)\n",
    "    solver.solve()\n",
    "\n",
    "\n",
    "def print_network_parameters(net):\n",
    "    '''\n",
    "    Print the parameters of the network\n",
    "    '''\n",
    "    print(net)\n",
    "    print('net.inputs: {0}'.format(net.inputs))\n",
    "    print('net.outputs: {0}'.format(net.outputs))\n",
    "    print('net.blobs: {0}'.format(net.blobs))\n",
    "    print('net.params: {0}'.format(net.params))    \n",
    "\n",
    "\n",
    "def get_predicted_output(deploy_prototxt_filename, \n",
    "                         caffemodel_filename, input, net=None):\n",
    "    '''\n",
    "    Get the predicted output, i.e. perform a forward pass\n",
    "    '''\n",
    "    if net is None:\n",
    "        net = caffe.Net(deploy_prototxt_filename, \n",
    "                        caffemodel_filename, caffe.TEST)\n",
    "    \n",
    "#     print \"Input: \"\n",
    "#     print input \n",
    "    out = net.forward(data=input)\n",
    "    return out[net.outputs[0]]\n",
    "\n",
    "\n",
    "def print_network(prototxt_filename, caffemodel_filename):\n",
    "    '''\n",
    "    Draw the ANN architecture\n",
    "    '''\n",
    "    _net = caffe.proto.caffe_pb2.NetParameter()\n",
    "    f = open(prototxt_filename)\n",
    "    google.protobuf.text_format.Merge(f.read(), _net)\n",
    "    caffe.draw.draw_net_to_file(_net, prototxt_filename + '.png')\n",
    "    print('Draw ANN done!')\n",
    "\n",
    "\n",
    "def print_network_weights(prototxt_filename, caffemodel_filename):\n",
    "    '''\n",
    "    For each ANN layer, print weight heatmap and weight histogram \n",
    "    '''\n",
    "    net = caffe.Net(prototxt_filename, caffemodel_filename, caffe.TEST)\n",
    "    for layer_name in net.params: \n",
    "        # weights heatmap \n",
    "        arr = net.params[layer_name][0].data\n",
    "        plt.clf()\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(arr, interpolation='none')\n",
    "        fig.colorbar(cax, orientation=\"horizontal\")\n",
    "        plt.savefig('{0}_weights_{1}.png'.format(caffemodel_filename, \n",
    "                                                 layer_name), \n",
    "                    dpi=100, format='png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # weights histogram  \n",
    "        plt.clf()\n",
    "        plt.hist(arr.tolist(), bins=20)\n",
    "        # savefig: use format='svg' or 'pdf' for vectorial pictures\n",
    "        plt.savefig('{0}_weights_hist_{1}.png'.format(caffemodel_filename, \n",
    "                                                      layer_name), dpi=100, \n",
    "                    format='png', \n",
    "                    bbox_inches='tight')  \n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def get_predicted_outputs(deploy_prototxt_filename, \n",
    "                          caffemodel_filename, inputs):\n",
    "    '''\n",
    "    Get several predicted outputs\n",
    "    '''\n",
    "    outputs = []\n",
    "    net = caffe.Net(deploy_prototxt_filename, caffemodel_filename, caffe.TRAIN)\n",
    "    outputs.append(copy.deepcopy(get_predicted_output(deploy_prototxt_filename, \n",
    "                                                      caffemodel_filename, \n",
    "                                                      inputs, net)))\n",
    "    return outputs    \n",
    "\n",
    "\n",
    "def get_accuracy(true_outputs, predicted_outputs):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    number_of_samples = true_outputs.shape[0]\n",
    "    number_of_outputs = true_outputs.shape[1]\n",
    "    threshold = 0.0  # 0 if SigmoidCrossEntropyLoss ; 0.5 if EuclideanLoss\n",
    "    for output_number in range(number_of_outputs):\n",
    "        predicted_output_binary = []\n",
    "        for sample_number in range(number_of_samples):\n",
    "            # print(predicted_outputs)\n",
    "            # print(predicted_outputs[sample_number][output_number])            \n",
    "            if predicted_outputs[sample_number][0][output_number] < threshold:\n",
    "                predicted_output = 0\n",
    "            else:\n",
    "                predicted_output = 1\n",
    "            predicted_output_binary.append(predicted_output)\n",
    "\n",
    "        print('accuracy: {0}'.format(sklearn.metrics.accuracy_score(\n",
    "                                     true_outputs[:, output_number], \n",
    "                                     predicted_output_binary)))\n",
    "        print(sklearn.metrics.confusion_matrix(true_outputs[:, output_number], \n",
    "                                               predicted_output_binary))\n",
    "\n",
    "\n",
    "def training(model_iter):\n",
    "    '''\n",
    "    Performs Training of the specified network and outputs PNG images\n",
    "    showing resulting learned weights and histograms of weights\n",
    "    '''\n",
    "    # Set parameters\n",
    "    solver_prototxt_filename = 'toycar_solver.prototxt'\n",
    "    train_test_prototxt_filename = 'toycar_2fc_hdf5.prototxt'\n",
    "    caffemodel_filename = '2fc_iter_' + str(model_iter) + '.caffemodel' \n",
    "\n",
    "    # Train network\n",
    "    train(solver_prototxt_filename)\n",
    "\n",
    "    # Print network\n",
    "    print_network(train_test_prototxt_filename, caffemodel_filename)\n",
    "    print_network_weights(train_test_prototxt_filename, caffemodel_filename)\n",
    "\n",
    "\n",
    "def testing(deploy_prototxt_filename, caffemodel_filename, inputs, labels):\n",
    "    '''\n",
    "    Performs Testing of the specified network\n",
    "    '''    \n",
    "    # Compute performance metrics\n",
    "    outputs = get_predicted_outputs(deploy_prototxt_filename, \n",
    "                                    caffemodel_filename, inputs)\n",
    "    \n",
    "#     print 'predictions: '\n",
    "#     print outputs[0]\n",
    "    \n",
    "#     print 'ground truths: '\n",
    "#     print labels\n",
    "    \n",
    "    return outputs[0]\n",
    "    \n",
    "    \n",
    "def euclidean_loss(pred, labels):\n",
    "    '''\n",
    "    Hand Calculate the Euclidean Loss to Compare with Model Output\n",
    "    '''\n",
    "    result = labels-pred\n",
    "    size = pred.shape[0]\n",
    "    \n",
    "    loss = np.sum(np.square(result), axis=1)\n",
    "    loss = np.sum(loss, axis=0) / (2 * size)\n",
    "    \n",
    "    print \"Euclidean Loss: \", loss\n",
    "\n",
    "\n",
    "def main(arg=\"x\"):\n",
    "\n",
    "    if arg.lower() == \"train\":\n",
    "        train_data, train_labels, test_data, test_labels = load_data()\n",
    "\n",
    "        # save_data_as_hdf5('toycar_hdf5_data_random_norm11_train.hdf5', \n",
    "        #                   train_data, train_labels)\n",
    "        # save_data_as_hdf5('toycar_hdf5_data_random_norm11_test.hdf5', \n",
    "        #                   test_data, test_labels)\n",
    "        \n",
    "        solver_name = \"toycar_solver.prototxt\"\n",
    "        training(20000)\n",
    "\n",
    "    elif arg.lower() == \"test\":\n",
    "        \n",
    "        train_data, train_labels, test_data, test_labels, meanx, minx, maxx = load_data()\n",
    "\n",
    "        pred = testing('toycar_2fc_deploy.prototxt', \n",
    "               '2fc_iter_100000.caffemodel', \n",
    "               test_data[0:1, :], test_labels[0:1, :])\n",
    "        print \"Input: \", test_data[0:1, :]\n",
    "#             pred[0, 0] = pred[0, 0] + test_data[0, 0]\n",
    "#             pred[0, 1] = pred[0, 1] + test_data[0, 1]\n",
    "        print \"Pred: \", pred\n",
    "        print \"Label: \", test_labels[0:1, :]\n",
    "        print \"Loss: \", euclidean_loss(pred, test_labels[0:1, :])\n",
    "\n",
    "\n",
    "        for i in range(1, 100):\n",
    "            \n",
    "#             print pred.shape\n",
    "#             print test_data[i:i+1,5:8].shape\n",
    "        \n",
    "#             print \"test: \", test_data[i-1,0:2].shape\n",
    "#             print \"meanx: \", np.asarray([meanx[0:2]]).shape\n",
    "#             print \"minx: \", np.asarray([minx[0:2]]).shape\n",
    "#             print \"maxx: \", np.asarray([maxx[0:2]]).shape\n",
    "            unnorm_state = unnormalize_data(test_data[i-1:i,0:2], np.asarray([meanx[0:2]])\n",
    "                                            , np.asarray([minx[0:2]]),np.asarray([maxx[0:2]]))\n",
    "        \n",
    "            pred[0, 0] = pred[0, 0] + unnorm_state[0,0]\n",
    "            pred[0, 1] = pred[0, 1] + unnorm_state[0,1]\n",
    "            \n",
    "            \n",
    "            pred = normalize_test_data(pred, \n",
    "                                       np.asarray([meanx[0:5]]), \n",
    "                                       np.asarray([minx[0:5]]), \n",
    "                                       np.asarray([maxx[0:5]]))\n",
    "            \n",
    "            inpt = np.asarray([np.concatenate((pred[0,:], test_data[i, 5:8]), axis=0)])\n",
    "            \n",
    "            print \"Test Data: \", test_data[i,:]\n",
    "            print \"Input: \", inpt\n",
    "            pred = testing('toycar_2fc_deploy.prototxt', \n",
    "                           '2fc_iter_100000.caffemodel', \n",
    "                           inpt,  test_labels[i:i+1, :])\n",
    "            \n",
    "            print \"Pred: \", pred\n",
    "            print \"Label: \", test_labels[i:i+1, :]\n",
    "            print \"Loss: \", euclidean_loss(pred, test_labels[i:i+1, :])\n",
    "            print \"\\n\"\n",
    "    else:\n",
    "        train_data, train_labels, test_data, test_labels = load_data()\n",
    "\n",
    "\n",
    "    # result = pred - test_labels[:1000, :]\n",
    "    # print np.sqrt(np.sum(np.square(result), axis=1)).shape\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [[-0.52753878 -0.09802863  0.13742372 -0.54690999  0.16745029 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.00133245 -0.0015051   0.13568482 -0.09937215  0.09792733]]\n",
      "Label:  [[-0.00196064 -0.00026777  0.13562027 -0.10375793  0.09954461]]\n",
      "Loss:  Euclidean Loss:  1.18902407849e-05\n",
      "None\n",
      "Test Data:  [-0.52800329 -0.09813368  0.13728903 -0.55064514  0.17081498 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.52785446 -0.0986191   0.13733354 -0.54844139  0.16810029 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.00139116 -0.00151112  0.13554779 -0.10242885  0.09835221]]\n",
      "Label:  [[-0.00210685 -0.00028731  0.13540609 -0.11119144  0.10154913]]\n",
      "Loss:  Euclidean Loss:  4.45166588179e-05\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52850244 -0.09824639  0.13714132 -0.55438028  0.17417968 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.52833288 -0.09872651  0.13723904 -0.54997731  0.16881347 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.00145112 -0.00151574  0.13540891 -0.10551107  0.09878758]]\n",
      "Label:  [[-0.002253   -0.00030673  0.13517247 -0.11862496  0.10355365]]\n",
      "Loss:  Euclidean Loss:  9.84250655165e-05\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52903622 -0.09836672  0.1369802  -0.55811543  0.17754437 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.52884624 -0.09884104  0.13714326 -0.55152604  0.16954425 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.00151173 -0.00152027  0.13526073 -0.10862654  0.09923665]]\n",
      "Label:  [[-0.00239915 -0.00032603  0.13491879 -0.12605846  0.10555818]]\n",
      "Loss:  Euclidean Loss:  0.000173082007677\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52960462 -0.09849463  0.13680525 -0.56185058  0.18090906 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.52939438 -0.09896315  0.13704107 -0.55309149  0.17029805 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.00157315 -0.00152449  0.13509248 -0.11178046  0.09969873]]\n",
      "Label:  [[-0.00254524 -0.0003452   0.13464448 -0.13349198  0.1075627 ]]\n",
      "Loss:  Euclidean Loss:  0.000267884111963\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53020763 -0.09863006  0.13661607 -0.56558573  0.18427376 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.52997732 -0.09909271  0.13692504 -0.55467625  0.17107367 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.00163532 -0.00152857  0.13490386 -0.11497325  0.10017504]]\n",
      "Label:  [[-0.00269121 -0.00036423  0.13434893 -0.14092548  0.10956722]]\n",
      "Loss:  Euclidean Loss:  0.00038225503522\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53084522 -0.09877295  0.13641224 -0.56932087  0.18763845 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.53059507 -0.09922974  0.13679496 -0.55628054  0.17187318 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.00169827 -0.00153252  0.13469456 -0.11820519  0.10066693]]\n",
      "Label:  [[-0.00283718 -0.0003831   0.13403155 -0.14835899  0.11157174]]\n",
      "Loss:  Euclidean Loss:  0.000515612191521\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.5315174  -0.09892324  0.13619336 -0.57305602  0.19100315 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.53124758 -0.09937418  0.13665061 -0.55790451  0.17269884 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.00176198 -0.00153633  0.13446434 -0.12147647  0.1011757 ]]\n",
      "Label:  [[-0.00298303 -0.0004018   0.13369176 -0.1557925   0.11357626]]\n",
      "Loss:  Euclidean Loss:  0.000667369517032\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53222415 -0.09908088  0.13595903 -0.57679117  0.19436784 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.53193485 -0.09952597  0.13649184 -0.55954825  0.17355285 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.00182646 -0.00154001  0.13421312 -0.12478745  0.10170279]]\n",
      "Label:  [[-0.00312895 -0.00042034  0.13332896 -0.16322601  0.11558078]]\n",
      "Loss:  Euclidean Loss:  0.000836926570628\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53296544 -0.09924579  0.13570882 -0.58052632  0.19773254 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.53265687 -0.09968505  0.13631858 -0.56121193  0.1744376  -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.00189172 -0.00154356  0.1339407  -0.12813818  0.10224967]]\n",
      "Label:  [[-0.00327468 -0.00043868  0.13294257 -0.17065951  0.1175853 ]]\n",
      "Loss:  Euclidean Loss:  0.00102368730586\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53374128 -0.09941789  0.13544235 -0.58426147  0.20109723 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.53341362 -0.09985135  0.13613071 -0.56289559  0.17535556 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.00195775 -0.00154698  0.13364708 -0.13152868  0.10281788]]\n",
      "Label:  [[-0.00342041 -0.00045682  0.132532   -0.17809303  0.11958983]]\n",
      "Loss:  Euclidean Loss:  0.0012270542793\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53455164 -0.09959711  0.13515919 -0.58799661  0.20446192 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.5342051  -0.10002479  0.13592822 -0.56459922  0.17630933 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.00202535 -0.00154852  0.13330141 -0.13496113  0.10340669]]\n",
      "Label:  [[-0.00356609 -0.00047476  0.13209665 -0.18552653  0.12159435]]\n",
      "Loss:  Euclidean Loss:  0.00144631485455\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53539652 -0.09978336  0.13485895 -0.59173176  0.20782662 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.53503149 -0.10020461  0.13568982 -0.56632394  0.17729768 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.00209428 -0.00154858  0.13288435 -0.13843727  0.10401689]]\n",
      "Label:  [[-0.0037117  -0.00049245  0.13163593 -0.19296005  0.12359887]]\n",
      "Loss:  Euclidean Loss:  0.0016807387583\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53627589 -0.09997656  0.13454121 -0.59546691  0.21119131 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.53589268 -0.10039089  0.1354022  -0.56807061  0.17832194 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.00216412 -0.00154802  0.13239457 -0.14195758  0.10465096]]\n",
      "Label:  [[-0.00385731 -0.00050991  0.13114925 -0.20039356  0.12560339]]\n",
      "Loss:  Euclidean Loss:  0.00192963168956\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53718975 -0.1001766   0.13420558 -0.59920206  0.21455601 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.5367886  -0.10058387  0.13506441 -0.56983948  0.17938625 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.00223488 -0.00154684  0.13183068 -0.14552283  0.10531153]]\n",
      "Label:  [[-0.00400275 -0.00052712  0.13063602 -0.20782706  0.12760791]]\n",
      "Loss:  Euclidean Loss:  0.00219226907939\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53813808 -0.1003834   0.13385163 -0.60293721  0.2179207  -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.53771924 -0.10078345  0.13467552 -0.57163093  0.18049505 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.00230644 -0.0015445   0.13120753 -0.14912158  0.1060071 ]]\n",
      "Label:  [[-0.00414819 -0.00054406  0.13009568 -0.21526058  0.12961243]]\n",
      "Loss:  Euclidean Loss:  0.00246860389598\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53912087 -0.10059684  0.13347897 -0.60667235  0.22128539 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.53868452 -0.10098933  0.13424577 -0.57343921  0.1816626  -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.00237811 -0.00154044  0.1305628  -0.15271592  0.10675158]]\n",
      "Label:  [[-0.00429356 -0.00056072  0.12952758 -0.22269408  0.13161695]]\n",
      "Loss:  Euclidean Loss:  0.0027604654897\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.5401381  -0.10081682  0.13308719 -0.6104075   0.22465009 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.53968428 -0.10120118  0.13380113 -0.57524527  0.18291225 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.00244968 -0.00153622  0.12989634 -0.15630674  0.10754764]]\n",
      "Label:  [[-0.004439   -0.00057708  0.12893119 -0.23012759  0.13362147]]\n",
      "Loss:  Euclidean Loss:  0.00306758517399\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.54118976 -0.10104322  0.13267589 -0.61414265  0.22801478 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.54071848 -0.1014195   0.13334151 -0.57704956  0.18424848 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.00252114 -0.00153185  0.12920776 -0.15989494  0.10839827]]\n",
      "Label:  [[-0.00458419 -0.00059313  0.1283059  -0.23756111  0.13562599]]\n",
      "Loss:  Euclidean Loss:  0.00338966632262\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.54227584 -0.10127591  0.13224465 -0.6178778   0.23137948 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.54178707 -0.10164419  0.13286662 -0.57885254  0.1856763  -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.00259253 -0.00152732  0.12849686 -0.16348112  0.10930641]]\n",
      "Label:  [[-0.00472939 -0.00060885  0.12765111 -0.24499461  0.13763052]]\n",
      "Loss:  Euclidean Loss:  0.00372641487047\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.54339632 -0.10151478  0.13179308 -0.62161295  0.23474417 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.54289004 -0.1018751   0.13237634 -0.5806545   0.18720067 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.00266384 -0.00152263  0.12776338 -0.1670661   0.11027513]]\n",
      "Label:  [[-0.00487447 -0.00062423  0.12696625 -0.25242811  0.13963504]]\n",
      "Loss:  Euclidean Loss:  0.00407750392333\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.54455119 -0.10175967  0.13132076 -0.62534809  0.23810886 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.54402744 -0.10211213  0.1318705  -0.58245586  0.18882671 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.0027351  -0.00151779  0.12700704 -0.17065072  0.1113077 ]]\n",
      "Label:  [[-0.00501966 -0.00063925  0.12625073 -0.25986162  0.14163956]]\n",
      "Loss:  Euclidean Loss:  0.00444258470088\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.54574043 -0.10201046  0.1308273  -0.62908324  0.24147356 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "Input:  [[-0.54519918 -0.10235512  0.13134889 -0.58425705  0.19055994 -0.37171077\n",
      "   0.50107972  0.02      ]]\n",
      "Pred:  [[-0.00280631 -0.0015128   0.12622762 -0.17423594  0.11240748]]\n",
      "Label:  [[-0.00516462 -0.0006539   0.12550396 -0.26729515  0.14364408]]\n",
      "Loss:  Euclidean Loss:  0.0048212823458\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.5264437  -0.09778059  0.13787201 -0.46100159  0.09006232 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.54640529 -0.10260395  0.13081136 -0.58605853  0.19240597 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.00291408 -0.00144186  0.12873963 -0.1877197   0.11841713]]\n",
      "Label:  [[ 0.00135124  0.00018559  0.13653767  0.05939361  0.05476179]]\n",
      "Loss:  Euclidean Loss:  0.0325993224978\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52612357 -0.09770778  0.13792171 -0.46866567  0.09564466 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.5271341  -0.09834626  0.13254377 -0.59283377  0.20249349 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.00320217 -0.00147315  0.13091335 -0.20192945  0.12353469]]\n",
      "Label:  [[ 0.00104922  0.00014418  0.13659716  0.04414092  0.05808748]]\n",
      "Loss:  Euclidean Loss:  0.0324434787035\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52587498 -0.09765122  0.13796274 -0.47632975  0.101227   -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.52688223 -0.09828572  0.13404289 -0.59997381  0.21108359 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.00348251 -0.00151863  0.13286249 -0.21655571  0.12853175]]\n",
      "Label:  [[  7.47382641e-04   1.02743506e-04   1.36642039e-01   2.88882405e-02\n",
      "    6.14131689e-02]]\n",
      "Loss:  Euclidean Loss:  0.0323912240565\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52569792 -0.09761091  0.1379937  -0.48399383  0.10680935 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.52670006 -0.098247    0.13538712 -0.60732313  0.21947142 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.003774   -0.00156331  0.13468762 -0.23172742  0.13343331]]\n",
      "Label:  [[  4.45544720e-04   6.12661242e-05   1.36670291e-01   1.36355562e-02\n",
      "    6.47388548e-02]]\n",
      "Loss:  Euclidean Loss:  0.0324731469154\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52559236 -0.09758687  0.13801318 -0.49165792  0.11239169 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.52659206 -0.09822422  0.13664582 -0.61494652  0.22769894 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.00407534 -0.0016083   0.13660634 -0.2473796   0.13829516]]\n",
      "Label:  [[  1.43945217e-04   1.97961926e-05   1.36679873e-01  -1.61712745e-03\n",
      "    6.80645406e-02]]\n",
      "Loss:  Euclidean Loss:  0.0326759926975\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52555826 -0.09757911  0.13801979 -0.499322    0.11797403 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.52655789 -0.09821783  0.13796907 -0.62281134  0.23585981 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.00438411 -0.00165284  0.13868311 -0.26334763  0.14311953]]\n",
      "Label:  [[ -1.57594681e-04  -2.16662884e-05   1.36668772e-01  -1.68698113e-02\n",
      "    7.13902265e-02]]\n",
      "Loss:  Euclidean Loss:  0.0329604931176\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52559559 -0.09758761  0.13801213 -0.50698608  0.12355637 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.52659693 -0.09822754  0.13940133 -0.63083486  0.24395777 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.00470555 -0.00169653  0.14083055 -0.27981555  0.14799401]]\n",
      "Label:  [[ -4.58955765e-04  -6.31138682e-05   1.36634961e-01  -3.21224965e-02\n",
      "    7.47159198e-02]]\n",
      "Loss:  Euclidean Loss:  0.033379919827\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52570432 -0.09761237  0.13798882 -0.51465016  0.12913871 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.52671043 -0.09825318  0.14088231 -0.63910958  0.25213984 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.00503501 -0.00174201  0.143053   -0.29670399  0.1529412 ]]\n",
      "Label:  [[ -7.60138035e-04  -1.04501843e-04   1.36576414e-01  -4.73751798e-02\n",
      "    7.80416057e-02]]\n",
      "Loss:  Euclidean Loss:  0.0339188575745\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52588442 -0.09765337  0.13794844 -0.52231425  0.13472105 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.52689721 -0.09829579  0.14241503 -0.64759558  0.26044395 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.00536926 -0.00179015  0.14530571 -0.31389046  0.15800172]]\n",
      "Label:  [[-0.00106126 -0.00014581  0.13649112 -0.06262786  0.08136729]]\n",
      "Loss:  Euclidean Loss:  0.034552346915\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52613585 -0.09771057  0.13788961 -0.52997833  0.14030339 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.52715649 -0.09835567  0.14396862 -0.65623134  0.26893831 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.00570257 -0.00184427  0.14759126 -0.33126312  0.16315828]]\n",
      "Label:  [[-0.00136214 -0.00018702  0.13637705 -0.07788055  0.08469298]]\n",
      "Loss:  Euclidean Loss:  0.0352534353733\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52645857 -0.09778394  0.13781094 -0.53764241  0.14588573 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.52748689 -0.09843411  0.14554486 -0.66496066  0.27759387 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.00603763 -0.00190108  0.14999384 -0.34882212  0.16837409]]\n",
      "Label:  [[-0.00166291 -0.0002281   0.13623217 -0.09313323  0.08801866]]\n",
      "Loss:  Euclidean Loss:  0.0360225662589\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52685255 -0.09787343  0.13771103 -0.54530649  0.15146807 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.527889   -0.09852976  0.1472018  -0.6737836   0.28634888 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.00637837 -0.00196031  0.15254004 -0.36663651  0.17363901]]\n",
      "Label:  [[-0.0019635  -0.000269    0.13605449 -0.10838591  0.09134436]]\n",
      "Loss:  Euclidean Loss:  0.0368799567223\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52731773 -0.09797896  0.13758848 -0.55297058  0.15705041 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.52836369 -0.09864249  0.1489578  -0.68273488  0.29518633 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.00672405 -0.00202156  0.1552331  -0.38471156  0.17895705]]\n",
      "Label:  [[-0.00226384 -0.00030973  0.13584195 -0.1236386   0.09467004]]\n",
      "Loss:  Euclidean Loss:  0.0378311164677\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52785408 -0.09810047  0.13744191 -0.56063466  0.16263275 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.52891079 -0.09877205  0.15081507 -0.69181713  0.30411292 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.00707553 -0.00208448  0.15809461 -0.40309066  0.18432902]]\n",
      "Label:  [[-0.00256407 -0.0003502   0.13559256 -0.13889128  0.09799573]]\n",
      "Loss:  Euclidean Loss:  0.038892224431\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52846156 -0.09823786  0.13726992 -0.56829874  0.1682151  -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.52953041 -0.09891825  0.15278853 -0.70105215  0.31313006 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.00743455 -0.00214946  0.16111094 -0.42184716  0.18977238]]\n",
      "Label:  [[-0.00286412 -0.0003904   0.13530432 -0.15414396  0.10132141]]\n",
      "Loss:  Euclidean Loss:  0.0400892719626\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52914012 -0.09839102  0.13707113 -0.57596282  0.17379744 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.53022294 -0.09908112  0.15486874 -0.71047682  0.32226703 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.00779597 -0.00220366  0.16408695 -0.44052535  0.19510876]]\n",
      "Label:  [[-0.00316393 -0.00043029  0.13497517 -0.16939665  0.1046471 ]]\n",
      "Loss:  Euclidean Loss:  0.0412830933928\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52988972 -0.09855983  0.13684413 -0.58362691  0.17937978 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.53098713 -0.09925555  0.15692116 -0.71986213  0.33122442 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.008151   -0.00225739  0.1670049  -0.45895261  0.20041545]]\n",
      "Label:  [[-0.00346363 -0.00046981  0.13460311 -0.18464933  0.10797279]]\n",
      "Loss:  Euclidean Loss:  0.0424314811826\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.5307103  -0.09874414  0.13658755 -0.59129099  0.18496212 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.53182085 -0.09944544  0.15893353 -0.72912135  0.34013198 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.00850081 -0.00231203  0.16989735 -0.47715968  0.20573013]]\n",
      "Label:  [[-0.00376296 -0.00050893  0.13418616 -0.19990201  0.11129848]]\n",
      "Loss:  Euclidean Loss:  0.0435450747609\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53160183 -0.09894381  0.13629999 -0.59895507  0.19054446 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.5327243  -0.09965119  0.16092831 -0.73826994  0.34905294 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.00884707 -0.00236822  0.1728763  -0.49516624  0.21104814]]\n",
      "Label:  [[-0.00406224 -0.0005476   0.13372226 -0.21515469  0.11462416]]\n",
      "Loss:  Euclidean Loss:  0.0446316450834\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53256425 -0.09915864  0.13598007 -0.60661915  0.1961268  -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.53369786 -0.09987289  0.16298276 -0.74731778  0.35797951 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.00918894 -0.00242561  0.1759221  -0.5129388   0.21638507]]\n",
      "Label:  [[-0.00436127 -0.00058575  0.13320944 -0.23040739  0.11794985]]\n",
      "Loss:  Euclidean Loss:  0.0456822738051\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53359751 -0.09938844  0.13562639 -0.61428324  0.20170914 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.53474128 -0.10011024  0.1650833  -0.75624803  0.36693782 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.00952719 -0.00248197  0.17889628 -0.53049749  0.22174378]]\n",
      "Label:  [[-0.00466007 -0.00062336  0.13264565 -0.24566007  0.12127554]]\n",
      "Loss:  Euclidean Loss:  0.0466962456703\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53470157 -0.09963299  0.13523758 -0.62194732  0.20729148 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.53585468 -0.10036215  0.16713445 -0.76507082  0.3759327  -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.00986171 -0.00253672  0.18178019 -0.54784828  0.22714068]]\n",
      "Label:  [[-0.00495863 -0.00066034  0.13202892 -0.26091275  0.12460123]]\n",
      "Loss:  Euclidean Loss:  0.0476745441556\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53587637 -0.09989205  0.13481225 -0.6296114   0.21287382 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.53703799 -0.10062818  0.16912335 -0.77378915  0.38499168 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.01019579 -0.0025684   0.18344751 -0.56457573  0.23231199]]\n",
      "Label:  [[-0.00525707 -0.00069665  0.13135722 -0.27616543  0.12792692]]\n",
      "Loss:  Euclidean Loss:  0.0484090186656\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53712185 -0.10016535  0.13434901 -0.63727548  0.21845616 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.53829193 -0.10089967  0.17027322 -0.78219426  0.39367201 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.01051953 -0.00258311  0.18366495 -0.58053094  0.23721863]]\n",
      "Label:  [[-0.00555515 -0.00073223  0.13062856 -0.29141811  0.1312526 ]]\n",
      "Loss:  Euclidean Loss:  0.0488279797137\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53843798 -0.10045262  0.13384648 -0.64493957  0.2240385  -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.53961412 -0.10117875  0.17042318 -0.79021135  0.40190806 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.01083455 -0.00257718  0.18259656 -0.59581262  0.24190734]]\n",
      "Label:  [[-0.00585318 -0.00076702  0.12984091 -0.30667081  0.13457829]]\n",
      "Loss:  Euclidean Loss:  0.0489668808877\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53982469 -0.10075353  0.13330328 -0.65260365  0.22962085 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.54100489 -0.10146368  0.16968637 -0.79789     0.4097783  -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.01114832 -0.00254531  0.18054682 -0.61045748  0.24647237]]\n",
      "Label:  [[-0.00615072 -0.00080094  0.12899229 -0.32192349  0.13790397]]\n",
      "Loss:  Euclidean Loss:  0.0488624274731\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.54128193 -0.10106775  0.13271802 -0.66026773  0.23520319 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.54246592 -0.10175209  0.16827275 -0.80524867  0.41744095 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.0114507  -0.00250559  0.17765293 -0.62452382  0.25095293]]\n",
      "Label:  [[-0.00644827 -0.00083394  0.1280807  -0.33717617  0.14122966]]\n",
      "Loss:  Euclidean Loss:  0.0485465452075\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.54280965 -0.10139492  0.13208934 -0.66793181  0.24078553 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.54399482 -0.10205073  0.16627697 -0.81231664  0.4249618  -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.01174264 -0.00245903  0.17403191 -0.63806039  0.25538304]]\n",
      "Label:  [[-0.00674558 -0.00086595  0.12710412 -0.35242885  0.14455535]]\n",
      "Loss:  Euclidean Loss:  0.0480489432812\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.54440778 -0.10173464  0.13141584 -0.6755959   0.24636787 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.5455917  -0.10235963  0.16377973 -0.81911842  0.43239798 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.01202521 -0.00240721  0.16981477 -0.65113252  0.25982645]]\n",
      "Label:  [[-0.00704253 -0.00089689  0.12606058 -0.36768153  0.14788103]]\n",
      "Loss:  Euclidean Loss:  0.0474088899791\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.54607629 -0.10208651  0.13069615 -0.68325998  0.25195021 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.54725679 -0.10267902  0.16087136 -0.82568683  0.43985647 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.01229908 -0.00235133  0.16513702 -0.66377586  0.26455253]]\n",
      "Label:  [[-0.00733924 -0.00092668  0.12494805 -0.38293421  0.15120673]]\n",
      "Loss:  Euclidean Loss:  0.0466805398464\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.5478151  -0.10245006  0.1299289  -0.69092406  0.25753255 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.54899017 -0.10300896  0.15764534 -0.83203979  0.44778945 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.01256468 -0.00229001  0.16002786 -0.67598027  0.26960057]]\n",
      "Label:  [[-0.00763583 -0.00095526  0.12376457 -0.39818689  0.15453242]]\n",
      "Loss:  Euclidean Loss:  0.0458754710853\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.54962416 -0.10282482  0.12911271 -0.69858814  0.26311489 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.5507919  -0.10334846  0.15412179 -0.83817219  0.45626284 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.01282184 -0.00222344  0.15451431 -0.68773764  0.27500093]]\n",
      "Label:  [[-0.00793207 -0.00098255  0.12250813 -0.4134396   0.1578581 ]]\n",
      "Loss:  Euclidean Loss:  0.0450058504939\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.55150341 -0.10321029  0.1282462  -0.70625223  0.26869723 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.55266189 -0.10369711  0.15031936 -0.84407997  0.46532763 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.01307034 -0.00215172  0.14861846 -0.6990363   0.28078425]]\n",
      "Label:  [[-0.00822806 -0.00100847  0.12117674 -0.42869228  0.16118379]]\n",
      "Loss:  Euclidean Loss:  0.0440839827061\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.5534528  -0.10360593  0.12732801 -0.71391631  0.27427957 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.55460003 -0.10405444  0.14625327 -0.84975725  0.47503523 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.01330903 -0.00207526  0.14234373 -0.70984071  0.28696167]]\n",
      "Label:  [[-0.00852382 -0.00103292  0.11976843 -0.44394496  0.16450948]]\n",
      "Loss:  Euclidean Loss:  0.0431143566966\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.55547226 -0.10401116  0.12635676 -0.72158039  0.27986191 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "Input:  [[-0.55660595 -0.10442008  0.14192588 -0.85518619  0.48540437 -0.76268556\n",
      "   0.83141642  0.02      ]]\n",
      "Pred:  [[-0.01353886 -0.00199557  0.13550436 -0.72029227  0.29350486]]\n",
      "Label:  [[-0.00881946 -0.00105584  0.11828118 -0.45919764  0.16783516]]\n",
      "Loss:  Euclidean Loss:  0.0421415381134\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.5264437  -0.09778059  0.13787201 -0.46100159  0.09006232 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.55867985 -0.10479405  0.13720909 -0.86043783  0.49638746 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[-0.01405905 -0.00123001  0.12369809 -0.72511834  0.30292565]]\n",
      "Label:  [[ 0.00132149  0.00018151  0.13653553  0.05578303  0.05376918]]\n",
      "Loss:  Euclidean Loss:  0.336144626141\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52613061 -0.09770939  0.13792024 -0.47047989  0.09397852 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.52977455 -0.09826315  0.12906686 -0.86286281  0.51220073 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[-0.01418403 -0.00106454  0.11260829 -0.73070723  0.31116134]]\n",
      "Label:  [[  9.48131084e-04   1.30280852e-04   1.36587948e-01   3.69197801e-02\n",
      "    5.61022609e-02]]\n",
      "Loss:  Euclidean Loss:  0.327555894852\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52590598 -0.09765827  0.13795639 -0.47995819  0.09789471 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.52949107 -0.09812702  0.12141875 -0.86567108  0.52602478 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[-0.01431915 -0.00094072  0.10190979 -0.73686069  0.31944555]]\n",
      "Label:  [[  5.74827194e-04   7.90208578e-05   1.36621058e-01   1.80565231e-02\n",
      "    5.84353358e-02]]\n",
      "Loss:  Euclidean Loss:  0.319727033377\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52576979 -0.09762727  0.13797923 -0.48943649  0.1018109  -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.52929846 -0.09802733  0.1140405  -0.86876304  0.53993026 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[-0.01446413 -0.00082703  0.09181821 -0.74359363  0.32779557]]\n",
      "Label:  [[  2.01702118e-04   2.77236104e-05   1.36633143e-01  -8.06732511e-04\n",
      "    6.07684143e-02]]\n",
      "Loss:  Euclidean Loss:  0.312630027533\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52572201 -0.0976164   0.13798756 -0.4989148   0.10572709 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.52919662 -0.09795173  0.10708081 -0.87214617  0.5539462  -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -1.46207111e-02  -7.17476942e-04   8.22956339e-02  -7.50919044e-01\n",
      "    3.36166441e-01]]\n",
      "Label:  [[ -1.71363354e-04  -2.35587358e-05   1.36622414e-01  -1.96699891e-02\n",
      "    6.31014928e-02]]\n",
      "Loss:  Euclidean Loss:  0.306225121021\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52576261 -0.09762564  0.13798016 -0.5083931   0.10964329 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.52918593 -0.09789787  0.10051354 -0.875827    0.56799716 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -1.47888577e-02  -6.12670556e-04   7.32727125e-02  -7.58826673e-01\n",
      "    3.44532877e-01]]\n",
      "Label:  [[ -5.44369221e-04  -7.48261809e-05   1.36587128e-01  -3.85332443e-02\n",
      "    6.54345676e-02]]\n",
      "Loss:  Euclidean Loss:  0.300465196371\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52589158 -0.097655    0.13795583 -0.5178714   0.11355948 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.52926636 -0.097866    0.09429086 -0.87980038  0.58204066 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -1.49685657e-02  -5.15353866e-04   6.46600574e-02  -7.67347276e-01\n",
      "    3.52859288e-01]]\n",
      "Label:  [[ -9.17196274e-04  -1.26041472e-04   1.36525527e-01  -5.73965013e-02\n",
      "    6.77676424e-02]]\n",
      "Loss:  Euclidean Loss:  0.295334815979\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52610888 -0.09770444  0.13791334 -0.5273497   0.11747567 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.52943791 -0.09785718  0.08835111 -0.88408177  0.59601698 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -1.51599096e-02  -4.24312428e-04   5.64345345e-02  -7.76512682e-01\n",
      "    3.61139745e-01]]\n",
      "Label:  [[-0.0012899  -0.00017716  0.13643585 -0.07625975  0.07010072]]\n",
      "Loss:  Euclidean Loss:  0.290825277567\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52641449 -0.09777395  0.1378515  -0.53682801  0.12139186 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.52970054 -0.09787091  0.08267836 -0.88868715  0.60991616 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -1.53636364e-02  -3.38425860e-04   4.85574827e-02  -7.86331952e-01\n",
      "    3.69365871e-01]]\n",
      "Label:  [[-0.00166255 -0.00022817  0.13631636 -0.09512302  0.0724338 ]]\n",
      "Loss:  Euclidean Loss:  0.28691393137\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52680838 -0.09786346  0.13776909 -0.54630631  0.12530806 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.53005441 -0.09790672  0.07724593 -0.89362108  0.62372414 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -1.55800516e-02  -2.56028958e-04   4.09705117e-02  -7.96782434e-01\n",
      "    3.77526522e-01]]\n",
      "Label:  [[-0.00203508 -0.00027902  0.13616528 -0.11398627  0.07476687]]\n",
      "Loss:  Euclidean Loss:  0.283559769392\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52729052 -0.09797292  0.13766489 -0.55578461  0.12922425 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.53049958 -0.0979639   0.07201355 -0.89887218  0.63742222 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -1.58086903e-02  -1.76759437e-04   3.36259156e-02  -8.07842910e-01\n",
      "    3.85601640e-01]]\n",
      "Label:  [[-0.00240749 -0.00032967  0.13598084 -0.13284953  0.07709996]]\n",
      "Loss:  Euclidean Loss:  0.280722767115\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52786089 -0.09810225  0.1375377  -0.56526291  0.13314044 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.53103589 -0.09804227  0.06694833 -0.90442979  0.65097673 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -1.60479620e-02  -1.01411715e-04   2.65116896e-02  -8.19448531e-01\n",
      "    3.93477172e-01]]\n",
      "Label:  [[-0.00277972 -0.00038008  0.13576134 -0.15171278  0.07943303]]\n",
      "Loss:  Euclidean Loss:  0.278303176165\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52851947 -0.09825137  0.13738631 -0.57474122  0.13705663 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.53166296 -0.09814204  0.06204198 -0.91026131  0.66419623 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -1.63013898e-02  -2.74702907e-05   1.95526760e-02  -8.31600964e-01\n",
      "    4.01143670e-01]]\n",
      "Label:  [[-0.00315189 -0.00043022  0.13550496 -0.17057604  0.08176611]]\n",
      "Loss:  Euclidean Loss:  0.27628698945\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52926621 -0.09842015  0.13720951 -0.58421952  0.14097283 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.53238157 -0.09826214  0.05724268 -0.9163676   0.67706485 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -1.65780000e-02   4.46923077e-05   1.28003042e-02  -8.44300330e-01\n",
      "    4.08584327e-01]]\n",
      "Label:  [[-0.00352401 -0.00048002  0.13521001 -0.1894393   0.08409919]]\n",
      "Loss:  Euclidean Loss:  0.274644196033\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53010111 -0.09860847  0.13700609 -0.59369782  0.14488902 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.53319385 -0.09840261  0.05258588 -0.9227487   0.68955438 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -1.68694593e-02   1.15949661e-04   6.31370209e-03  -8.57679546e-01\n",
      "    4.15809631e-01]]\n",
      "Label:  [[-0.00389588 -0.00052945  0.13487469 -0.20830254  0.08643226]]\n",
      "Loss:  Euclidean Loss:  0.27343827486\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53102412 -0.09881618  0.13677484 -0.60317612  0.14880521 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.53409779 -0.09856298  0.04811238 -0.92947142  0.70168244 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -1.71736218e-02   1.83684751e-04   5.23086637e-05  -8.71720135e-01\n",
      "    4.22829956e-01]]\n",
      "Label:  [[-0.00426769 -0.00057844  0.13449727 -0.2271658   0.08876534]]\n",
      "Loss:  Euclidean Loss:  0.272646009922\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53203522 -0.09904311  0.13651455 -0.61265443  0.1527214  -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.53509286 -0.09874411  0.04379419 -0.93652645  0.71346642 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -1.74894519e-02   2.46453099e-04  -6.15433790e-03  -8.86490405e-01\n",
      "    4.29669350e-01]]\n",
      "Label:  [[-0.00463945 -0.00062696  0.13407598 -0.24602906  0.09109842]]\n",
      "Loss:  Euclidean Loss:  0.272325694561\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53313438 -0.09928907  0.13622401 -0.62213273  0.1566376  -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.5361788  -0.09894642  0.03951376 -0.94394814  0.7249467  -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -1.78206787e-02   3.10067087e-04  -1.25232954e-02  -9.01926219e-01\n",
      "    4.36414331e-01]]\n",
      "Label:  [[-0.00501102 -0.00067493  0.1336091  -0.26489231  0.0934315 ]]\n",
      "Loss:  Euclidean Loss:  0.272484600544\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53432158 -0.09955385  0.13590202 -0.63161103  0.16055379 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.53735643 -0.09916743  0.03512139 -0.95170424  0.73626851 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -1.81650147e-02   3.75060365e-04  -1.90883297e-02  -9.17986214e-01\n",
      "    4.43062752e-01]]\n",
      "Label:  [[-0.00538248 -0.00072229  0.13309486 -0.28375557  0.09576457]]\n",
      "Loss:  Euclidean Loss:  0.273094415665\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.5355968  -0.09983722  0.13554738 -0.64108933  0.16446998 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.53862521 -0.09940671  0.0305938  -0.95977397  0.74742823 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -1.85220465e-02   4.40398231e-04  -2.58528162e-02  -9.34676111e-01\n",
      "    4.49625701e-01]]\n",
      "Label:  [[-0.00575387 -0.000769    0.13253152 -0.30261883  0.09809764]]\n",
      "Loss:  Euclidean Loss:  0.274159252644\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53696    -0.10013891  0.13515886 -0.65056764  0.16838617 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.53998501 -0.09966444  0.02592865 -0.96816022  0.75844448 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -1.88896507e-02   5.03600575e-04  -3.28419730e-02  -9.51961458e-01\n",
      "    4.56109583e-01]]\n",
      "Label:  [[-0.00612515 -0.00081497  0.13191731 -0.32148209  0.10043073]]\n",
      "Loss:  Euclidean Loss:  0.275660961866\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53841116 -0.10045863  0.13473527 -0.66004594  0.17230237 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.54143531 -0.09994134  0.02110856 -0.97684566  0.76932802 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -1.92697272e-02   5.67777082e-04  -4.00551185e-02  -9.69854653e-01\n",
      "    4.62526053e-01]]\n",
      "Label:  [[-0.00649625 -0.00086015  0.1312505  -0.34034535  0.1027638 ]]\n",
      "Loss:  Euclidean Loss:  0.277610838413\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.53995025 -0.10079608  0.13427541 -0.66952424  0.17621856 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.54297652 -0.10023589  0.01613399 -0.98583653  0.7800984  -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -1.96596533e-02   6.32423908e-04  -4.75029275e-02  -9.88318145e-01\n",
      "    4.68833953e-01]]\n",
      "Label:  [[-0.00686741 -0.00090446  0.13052934 -0.35920858  0.10509688]]\n",
      "Loss:  Euclidean Loss:  0.279972493649\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.54157725 -0.10115092  0.13377805 -0.67900254  0.18013475 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.54460797 -0.10054797  0.01099759 -0.99511396  0.79068655 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -2.00586468e-02   6.98552467e-04  -5.52128926e-02  -1.00725222e+00\n",
      "    4.75012988e-01]]\n",
      "Label:  [[-0.00723827 -0.00094784  0.12975208 -0.37807184  0.10742996]]\n",
      "Loss:  Euclidean Loss:  0.282682180405\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.54329214 -0.10152277  0.13324201 -0.68848085  0.18405094 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.54632952 -0.10087687  0.00568039 -1.00462785  0.80105838 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -2.04656385e-02   7.63427466e-04  -6.31034896e-02  -1.02662683e+00\n",
      "    4.81068105e-01]]\n",
      "Label:  [[-0.00760913 -0.0009902   0.12891696 -0.39693511  0.10976303]]\n",
      "Loss:  Euclidean Loss:  0.285709679127\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.54509489 -0.10191124  0.13266608 -0.69795915  0.18796714 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[ -5.48140820e-01  -1.01223265e-01   2.38612693e-04  -1.01436309e+00\n",
      "    8.11222212e-01  -9.43221595e-01   5.83229941e-01   2.00000000e-02]]\n",
      "Pred:  [[ -2.08818652e-02   8.29707831e-04  -7.12151676e-02  -1.04642439e+00\n",
      "    4.87101495e-01]]\n",
      "Label:  [[-0.00797999 -0.00103147  0.12802227 -0.41579837  0.11209611]]\n",
      "Loss:  Euclidean Loss:  0.289091855288\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.54698548 -0.1023159   0.13204905 -0.70743745  0.19188333 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.55004219 -0.10158573 -0.00535563 -1.02431086  0.82134958 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -2.13077627e-02   8.99050385e-04  -7.96459615e-02  -1.06664944e+00\n",
      "    4.93224621e-01]]\n",
      "Label:  [[-0.00835061 -0.00107157  0.12706624 -0.43466163  0.11442919]]\n",
      "Loss:  Euclidean Loss:  0.292898118496\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.54896389 -0.1027363   0.13138971 -0.71691575  0.19579952 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.55203368 -0.10196319 -0.01116995 -1.03447343  0.83162756 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -2.17419639e-02   9.69870016e-04  -8.85258168e-02  -1.08736610e+00\n",
      "    4.99399722e-01]]\n",
      "Label:  [[-0.00872111 -0.00111042  0.12604712 -0.45352489  0.11676227]]\n",
      "Loss:  Euclidean Loss:  0.297190755606\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.55103009 -0.10317193  0.13068688 -0.72639406  0.19971571 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.55411495 -0.1023558  -0.01729397 -1.04488302  0.84199279 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -2.21870542e-02   1.04401167e-03  -9.78518799e-02  -1.10859871e+00\n",
      "    5.05639434e-01]]\n",
      "Label:  [[-0.00909162 -0.00114793  0.12496318 -0.47238812  0.11909534]]\n",
      "Loss:  Euclidean Loss:  0.302001535892\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.55318407 -0.10362228  0.12993934 -0.73587236  0.20363191 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.55628661 -0.10276235 -0.02372572 -1.05555186  0.85246648 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[ -2.26460025e-02   1.12657808e-03  -1.07598186e-01  -1.13030696e+00\n",
      "    5.12054741e-01]]\n",
      "Label:  [[-0.009462   -0.001184    0.12381268 -0.49125138  0.12142842]]\n",
      "Loss:  Euclidean Loss:  0.307355552912\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.55542579 -0.10408678  0.12914589 -0.74535066  0.2075481  -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.55854931 -0.1031803  -0.03044728 -1.0664597   0.86323491 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[-0.02311321  0.00121828 -0.11768138 -1.15238285  0.51877928]]\n",
      "Label:  [[-0.00983226 -0.00121857  0.12259386 -0.51011467  0.1237615 ]]\n",
      "Loss:  Euclidean Loss:  0.313230991364\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.55775525 -0.10456484  0.12830533 -0.75482896  0.21146429 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.56090175 -0.10360883 -0.03740119 -1.07755227  0.8745224  -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[-0.02358873  0.0013094  -0.12816173 -1.17489552  0.5258038 ]]\n",
      "Label:  [[-0.01020253 -0.00125153  0.121305   -0.52897787  0.12609458]]\n",
      "Loss:  Euclidean Loss:  0.319698244333\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.56017242 -0.10505583  0.12741647 -0.76430727  0.21538048 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.56334386 -0.10405114 -0.04462899 -1.08886431  0.88631344 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[-0.02407191  0.00140124 -0.13912426 -1.19786739  0.53312939]]\n",
      "Label:  [[-0.01057267 -0.00128278  0.11994436 -0.54784113  0.12842765]]\n",
      "Loss:  Euclidean Loss:  0.326811790466\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.56267727 -0.10555909  0.12647809 -0.77378557  0.21929668 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "Input:  [[-0.56587549 -0.10450611 -0.05218933 -1.10040709  0.89860982 -0.94322159\n",
      "   0.58322994  0.02      ]]\n",
      "Pred:  [[-0.02456351  0.00149753 -0.15071493 -1.2212677   0.54079247]]\n",
      "Label:  [[-0.0109427  -0.00131223  0.11851019 -0.56670439  0.13076073]]\n",
      "Loss:  Euclidean Loss:  0.334627389908\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.5264437  -0.09778059  0.13787201 -0.46100159  0.09006232 -0.86892124\n",
      "  0.64540666  0.02      ]\n",
      "Input:  [[-0.56849683 -0.10497158 -0.06018287 -1.11216514  0.91147269 -0.86892124\n",
      "   0.64540666  0.02      ]]\n",
      "Pred:  [[-0.02502315  0.00152299 -0.16617364 -1.24448729  0.55072439]]\n",
      "Label:  [[ 0.00133371  0.00018319  0.13653632  0.05726898  0.05401786]]\n",
      "Loss:  Euclidean Loss:  1.01680827141\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52612771 -0.09770873  0.13792078 -0.46973324  0.09439593 -0.86892124\n",
      "  0.64540666  0.02      ]\n",
      "Input:  [[-0.53237215 -0.0971831  -0.07084401 -1.12383239  0.92814394 -0.86892124\n",
      "   0.64540666  0.02      ]]\n",
      "Pred:  [[-0.02560603  0.00169431 -0.18144509 -1.27011657  0.56067795]]\n",
      "Label:  [[  9.89794731e-04   1.36010349e-04   1.36591390e-01   3.98916714e-02\n",
      "    5.65996058e-02]]\n",
      "Loss:  Euclidean Loss:  1.03603672981\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52589322 -0.09765537  0.13795876 -0.47846489  0.09872953 -0.86892124\n",
      "  0.64540666  0.02      ]\n",
      "Input:  [[-0.53219425 -0.09704402 -0.08137602 -1.13671045  0.94485151 -0.86892124\n",
      "   0.64540666  0.02      ]]\n",
      "Pred:  [[-0.02618654  0.00185125 -0.19684909 -1.29682565  0.57101142]]\n",
      "Label:  [[  6.45875931e-04   8.87811184e-05   1.36629000e-01   2.25143619e-02\n",
      "    5.91813549e-02]]\n",
      "Loss:  Euclidean Loss:  1.05727934837\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.5257402  -0.09762054  0.1379847  -0.48719654  0.10306313 -0.86892124\n",
      "  0.64540666  0.02      ]\n",
      "Input:  [[-0.53209731 -0.0969291  -0.09199943 -1.15013107  0.9621968  -0.86892124\n",
      "   0.64540666  0.02      ]]\n",
      "Pred:  [[-0.02678015  0.00199982 -0.21265922 -1.324512    0.58188313]]\n",
      "Label:  [[  3.02076340e-04   4.15369868e-05   1.36647359e-01   5.13705146e-03\n",
      "    6.17631078e-02]]\n",
      "Loss:  Euclidean Loss:  1.08062195778\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52566864 -0.09760424  0.13799736 -0.4959282   0.10739674 -0.86892124\n",
      "  0.64540666  0.02      ]\n",
      "Input:  [[-0.53208492 -0.09683598 -0.10290293 -1.16404276  0.98044553 -0.86892124\n",
      "   0.64540666  0.02      ]]\n",
      "Pred:  [[-0.0273906   0.00214894 -0.22898965 -1.35311127  0.59339482]]\n",
      "Label:  [[ -4.16040421e-05  -5.72204590e-06   1.36644691e-01  -1.22402580e-02\n",
      "    6.43448532e-02]]\n",
      "Loss:  Euclidean Loss:  1.10613501072\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52567849 -0.09760649  0.13799552 -0.50465985  0.11173034 -0.86892124\n",
      "  0.64540666  0.02      ]\n",
      "Input:  [[-0.53215797 -0.09676119 -0.11416525 -1.17841316  0.9997685  -0.86892124\n",
      "   0.64540666  0.02      ]]\n",
      "Pred:  [[-0.02801725  0.00229937 -0.24561238 -1.38260698  0.60573953]]\n",
      "Label:  [[ -3.85165215e-04  -5.29512763e-05   1.36619180e-01  -2.96175685e-02\n",
      "    6.69266060e-02]]\n",
      "Loss:  Euclidean Loss:  1.13388490677\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52576974 -0.09762726  0.13797793 -0.5133915   0.11606394 -0.86892124\n",
      "  0.64540666  0.02      ]\n",
      "Input:  [[-0.5323163  -0.09670441 -0.12562917 -1.193234    1.02048976 -0.86892124\n",
      "   0.64540666  0.02      ]]\n",
      "Pred:  [[-0.02865857  0.00245276 -0.26249793 -1.41283345  0.61915255]]\n",
      "Label:  [[ -7.28607178e-04  -1.00150704e-04   1.36569068e-01  -4.69948761e-02\n",
      "    6.95083588e-02]]\n",
      "Loss:  Euclidean Loss:  1.16383230686\n",
      "None\n",
      "\n",
      "\n",
      "Test Data:  [-0.52594236 -0.09766655  0.13794337 -0.52212315  0.12039754 -0.86892124\n",
      "  0.64540666  0.02      ]\n",
      "Input:  [[-0.53255949 -0.09666501 -0.13727434 -1.20842202  1.0430042  -0.86892124\n",
      "   0.64540666  0.02      ]]\n",
      "Pred:  [[-0.0293138   0.00260717 -0.27934617 -1.4437995   0.63363075]]\n",
      "Label:  [[-0.00107193 -0.00014728  0.13649254 -0.06437219  0.0720901 ]]\n",
      "Loss:  Euclidean Loss:  1.19593727589\n",
      "None\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# main()\n",
    "main(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data, test_labels, meanx, minx, maxx = load_data()\n",
    "check = np.asarray([[0, 0, 0.13562027, -0.10375793,  0.09954461]])\n",
    "res = normalize_test_data(check,\n",
    "                          np.asarray([meanx[0:5]]), \n",
    "                           np.asarray([minx[0:5]]), \n",
    "                           np.asarray([maxx[0:5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.52753878 -0.09802863  0.13742372 -0.54690999  0.16745029 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "[-0.52800329 -0.09813368  0.13728903 -0.55064514  0.17081498 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "[-0.52850244 -0.09824639  0.13714132 -0.55438028  0.17417968 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "[-0.52903622 -0.09836672  0.1369802  -0.55811543  0.17754437 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "[-0.52960462 -0.09849463  0.13680525 -0.56185058  0.18090906 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "[-0.53020763 -0.09863006  0.13661607 -0.56558573  0.18427376 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "[-0.53084522 -0.09877295  0.13641224 -0.56932087  0.18763845 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "[-0.5315174  -0.09892324  0.13619336 -0.57305602  0.19100315 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "[-0.53222415 -0.09908088  0.13595903 -0.57679117  0.19436784 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "[-0.53296544 -0.09924579  0.13570882 -0.58052632  0.19773254 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "[-0.53374128 -0.09941789  0.13544235 -0.58426147  0.20109723 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "[-0.53455164 -0.09959711  0.13515919 -0.58799661  0.20446192 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "[-0.53539652 -0.09978336  0.13485895 -0.59173176  0.20782662 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "[-0.53627589 -0.09997656  0.13454121 -0.59546691  0.21119131 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "[-0.53718975 -0.1001766   0.13420558 -0.59920206  0.21455601 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "[-0.53813808 -0.1003834   0.13385163 -0.60293721  0.2179207  -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "[-0.53912087 -0.10059684  0.13347897 -0.60667235  0.22128539 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "[-0.5401381  -0.10081682  0.13308719 -0.6104075   0.22465009 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "[-0.54118976 -0.10104322  0.13267589 -0.61414265  0.22801478 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "[-0.54227584 -0.10127591  0.13224465 -0.6178778   0.23137948 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "[-0.54339632 -0.10151478  0.13179308 -0.62161295  0.23474417 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "[-0.54455119 -0.10175967  0.13132076 -0.62534809  0.23810886 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "[-0.54574043 -0.10201046  0.1308273  -0.62908324  0.24147356 -0.37171077\n",
      "  0.50107972  0.02      ]\n",
      "[-0.5264437  -0.09778059  0.13787201 -0.46100159  0.09006232 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.52612357 -0.09770778  0.13792171 -0.46866567  0.09564466 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.52587498 -0.09765122  0.13796274 -0.47632975  0.101227   -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.52569792 -0.09761091  0.1379937  -0.48399383  0.10680935 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.52559236 -0.09758687  0.13801318 -0.49165792  0.11239169 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.52555826 -0.09757911  0.13801979 -0.499322    0.11797403 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.52559559 -0.09758761  0.13801213 -0.50698608  0.12355637 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.52570432 -0.09761237  0.13798882 -0.51465016  0.12913871 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.52588442 -0.09765337  0.13794844 -0.52231425  0.13472105 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.52613585 -0.09771057  0.13788961 -0.52997833  0.14030339 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.52645857 -0.09778394  0.13781094 -0.53764241  0.14588573 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.52685255 -0.09787343  0.13771103 -0.54530649  0.15146807 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.52731773 -0.09797896  0.13758848 -0.55297058  0.15705041 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.52785408 -0.09810047  0.13744191 -0.56063466  0.16263275 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.52846156 -0.09823786  0.13726992 -0.56829874  0.1682151  -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.52914012 -0.09839102  0.13707113 -0.57596282  0.17379744 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.52988972 -0.09855983  0.13684413 -0.58362691  0.17937978 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.5307103  -0.09874414  0.13658755 -0.59129099  0.18496212 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.53160183 -0.09894381  0.13629999 -0.59895507  0.19054446 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.53256425 -0.09915864  0.13598007 -0.60661915  0.1961268  -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.53359751 -0.09938844  0.13562639 -0.61428324  0.20170914 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.53470157 -0.09963299  0.13523758 -0.62194732  0.20729148 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.53587637 -0.09989205  0.13481225 -0.6296114   0.21287382 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.53712185 -0.10016535  0.13434901 -0.63727548  0.21845616 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.53843798 -0.10045262  0.13384648 -0.64493957  0.2240385  -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.53982469 -0.10075353  0.13330328 -0.65260365  0.22962085 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.54128193 -0.10106775  0.13271802 -0.66026773  0.23520319 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.54280965 -0.10139492  0.13208934 -0.66793181  0.24078553 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.54440778 -0.10173464  0.13141584 -0.6755959   0.24636787 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.54607629 -0.10208651  0.13069615 -0.68325998  0.25195021 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.5478151  -0.10245006  0.1299289  -0.69092406  0.25753255 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.54962416 -0.10282482  0.12911271 -0.69858814  0.26311489 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.55150341 -0.10321029  0.1282462  -0.70625223  0.26869723 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.5534528  -0.10360593  0.12732801 -0.71391631  0.27427957 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.55547226 -0.10401116  0.12635676 -0.72158039  0.27986191 -0.76268556\n",
      "  0.83141642  0.02      ]\n",
      "[-0.5264437  -0.09778059  0.13787201 -0.46100159  0.09006232 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.52613061 -0.09770939  0.13792024 -0.47047989  0.09397852 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.52590598 -0.09765827  0.13795639 -0.47995819  0.09789471 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.52576979 -0.09762727  0.13797923 -0.48943649  0.1018109  -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.52572201 -0.0976164   0.13798756 -0.4989148   0.10572709 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.52576261 -0.09762564  0.13798016 -0.5083931   0.10964329 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.52589158 -0.097655    0.13795583 -0.5178714   0.11355948 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.52610888 -0.09770444  0.13791334 -0.5273497   0.11747567 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.52641449 -0.09777395  0.1378515  -0.53682801  0.12139186 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.52680838 -0.09786346  0.13776909 -0.54630631  0.12530806 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.52729052 -0.09797292  0.13766489 -0.55578461  0.12922425 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.52786089 -0.09810225  0.1375377  -0.56526291  0.13314044 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.52851947 -0.09825137  0.13738631 -0.57474122  0.13705663 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.52926621 -0.09842015  0.13720951 -0.58421952  0.14097283 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.53010111 -0.09860847  0.13700609 -0.59369782  0.14488902 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.53102412 -0.09881618  0.13677484 -0.60317612  0.14880521 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.53203522 -0.09904311  0.13651455 -0.61265443  0.1527214  -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.53313438 -0.09928907  0.13622401 -0.62213273  0.1566376  -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.53432158 -0.09955385  0.13590202 -0.63161103  0.16055379 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.5355968  -0.09983722  0.13554738 -0.64108933  0.16446998 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.53696    -0.10013891  0.13515886 -0.65056764  0.16838617 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.53841116 -0.10045863  0.13473527 -0.66004594  0.17230237 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.53995025 -0.10079608  0.13427541 -0.66952424  0.17621856 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.54157725 -0.10115092  0.13377805 -0.67900254  0.18013475 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.54329214 -0.10152277  0.13324201 -0.68848085  0.18405094 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.54509489 -0.10191124  0.13266608 -0.69795915  0.18796714 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.54698548 -0.1023159   0.13204905 -0.70743745  0.19188333 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.54896389 -0.1027363   0.13138971 -0.71691575  0.19579952 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.55103009 -0.10317193  0.13068688 -0.72639406  0.19971571 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.55318407 -0.10362228  0.12993934 -0.73587236  0.20363191 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.55542579 -0.10408678  0.12914589 -0.74535066  0.2075481  -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.55775525 -0.10456484  0.12830533 -0.75482896  0.21146429 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.56017242 -0.10505583  0.12741647 -0.76430727  0.21538048 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.56267727 -0.10555909  0.12647809 -0.77378557  0.21929668 -0.94322159\n",
      "  0.58322994  0.02      ]\n",
      "[-0.5264437  -0.09778059  0.13787201 -0.46100159  0.09006232 -0.86892124\n",
      "  0.64540666  0.02      ]\n",
      "[-0.52612771 -0.09770873  0.13792078 -0.46973324  0.09439593 -0.86892124\n",
      "  0.64540666  0.02      ]\n",
      "[-0.52589322 -0.09765537  0.13795876 -0.47846489  0.09872953 -0.86892124\n",
      "  0.64540666  0.02      ]\n",
      "[-0.5257402  -0.09762054  0.1379847  -0.48719654  0.10306313 -0.86892124\n",
      "  0.64540666  0.02      ]\n",
      "[-0.52566864 -0.09760424  0.13799736 -0.4959282   0.10739674 -0.86892124\n",
      "  0.64540666  0.02      ]\n",
      "[-0.52567849 -0.09760649  0.13799552 -0.50465985  0.11173034 -0.86892124\n",
      "  0.64540666  0.02      ]\n",
      "[-0.52576974 -0.09762726  0.13797793 -0.5133915   0.11606394 -0.86892124\n",
      "  0.64540666  0.02      ]\n",
      "[-0.52594236 -0.09766655  0.13794337 -0.52212315  0.12039754 -0.86892124\n",
      "  0.64540666  0.02      ]\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for x in test_data:\n",
    "    if i < 100:\n",
    "        print test_data[i,:]\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.95341616  0.86100332  1.85468962 -1.04898425  1.99619798 -0.29372734\n",
      "   3.39019925  1.35962904]]\n",
      "[-0.00196064 -0.00026777  0.13562027 -0.10375793  0.09954461]\n"
     ]
    }
   ],
   "source": [
    "print unnormalize_data(test_data[1:2,:], meanx, minx, maxx)\n",
    "print test_labels[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.29162416  0.00804919  0.03366669  0.27424605  0.00841767  0.05690521\n",
      "  0.00058458  0.        ]\n",
      "[-3.23727242 -2.3730294  -1.54712097 -1.27228815 -0.6063877  -1.05684408\n",
      " -0.20053342  0.02      ] [ 5.20444778  2.72492031  1.35288862  2.70801442  0.58511506  0.94307371\n",
      "  0.19941189  0.02      ]\n",
      "norm_a: [[-0.06520279  0.71244538  2.1127168   1.51138802  8.39648689]]\n",
      "unnorm_a:  [[ 1.  2.  3.  4.  5.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.asarray([[1, 2, 3, 4, 5]])\n",
    "print meanx\n",
    "print minx,maxx\n",
    "norm_a = normalize_test_data(a, np.asarray([meanx[0:5]]), np.asarray([minx]), np.asarray([maxx]))\n",
    "print 'norm_a:', norm_a\n",
    "unnorm_a = unnormalize_data(norm_a, np.asarray([meanx[0:5]]), np.asarray([minx[0:5]]), \n",
    "                            np.asarray([maxx[0:5]]))\n",
    "print 'unnorm_a: ',unnorm_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29162415862083435, 0.008049188181757927, 0.033666692674160004, 0.27424606680870056, 0.008417674340307713, 0.05690521001815796, 0.0005845790728926659, 0.0]\n",
      "[0.6776861548423767, -0.06710384786128998, -0.05981121584773064, 0.4085806608200073, -0.004740194883197546, 0.04174768552184105, 0.0003070685488637537, 0.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "blob = caffe.proto.caffe_pb2.BlobProto()\n",
    "data = open( \"toycar_mean.binaryproto\" , 'rb' ).read()\n",
    "blob.ParseFromString(data)\n",
    "# arr = np.array( caffe.io.blobproto_to_array(blob) )\n",
    "# out = arr[0]\n",
    "print blob.data\n",
    "\n",
    "data = open( \"../prx_ws/src/prx_learn/data/toy_car/toy_car_mean.binaryproto\" , 'rb' ).read()\n",
    "blob.ParseFromString(data)\n",
    "print blob.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
